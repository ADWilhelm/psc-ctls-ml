{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7ae2b9fa-bc6c-4326-bf63-4cbb898ddcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9829edfd-ae90-408d-b3af-6a0f62a746b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reduced_formulas</th>\n",
       "      <th>etl</th>\n",
       "      <th>htl</th>\n",
       "      <th>bandgap</th>\n",
       "      <th>device_stack</th>\n",
       "      <th>pce</th>\n",
       "      <th>etl_SMILES</th>\n",
       "      <th>htl_SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ag20Bi20CsI60</td>\n",
       "      <td>['TiO2-c', 'TiO2-mp']</td>\n",
       "      <td>['P3HT']</td>\n",
       "      <td>1.86</td>\n",
       "      <td>['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...</td>\n",
       "      <td>3.53</td>\n",
       "      <td>[O=[Ti]=O, O=[Ti]=O]</td>\n",
       "      <td>[CCCCCCC1=CSC=C1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ag20Bi20CsI60</td>\n",
       "      <td>['TiO2-c', 'TiO2-mp']</td>\n",
       "      <td>['PTB7-th']</td>\n",
       "      <td>1.86</td>\n",
       "      <td>['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...</td>\n",
       "      <td>3.53</td>\n",
       "      <td>[O=[Ti]=O, O=[Ti]=O]</td>\n",
       "      <td>[CCC(=CF)COC1=CC=C(C=C1)C23CCC(CC2)(CC3)C(=O)N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ag2BiI5</td>\n",
       "      <td>['TiO2-c', 'TiO2-mp']</td>\n",
       "      <td>['PTAA']</td>\n",
       "      <td>2.22</td>\n",
       "      <td>['SLG', 'ITO', 'TiO2-c', 'TiO2-mp', 'Perovskit...</td>\n",
       "      <td>2.60</td>\n",
       "      <td>[O=[Ti]=O, O=[Ti]=O]</td>\n",
       "      <td>[CC1=CC(=C(C(=C1)C)N(C2=CC=CC=C2)C3=CC=CC=C3)C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ag3BiI6</td>\n",
       "      <td>['TiO2-c', 'TiO2-mp']</td>\n",
       "      <td>['P3HT']</td>\n",
       "      <td>1.80</td>\n",
       "      <td>['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...</td>\n",
       "      <td>2.32</td>\n",
       "      <td>[O=[Ti]=O, O=[Ti]=O]</td>\n",
       "      <td>[CCCCCCC1=CSC=C1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ag3BiI6</td>\n",
       "      <td>['TiO2-c', 'TiO2-mp']</td>\n",
       "      <td>['PTAA']</td>\n",
       "      <td>0.00</td>\n",
       "      <td>['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>[O=[Ti]=O, O=[Ti]=O]</td>\n",
       "      <td>[CC1=CC(=C(C(=C1)C)N(C2=CC=CC=C2)C3=CC=CC=C3)C]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reduced_formulas                    etl          htl  bandgap  \\\n",
       "0    Ag20Bi20CsI60  ['TiO2-c', 'TiO2-mp']     ['P3HT']     1.86   \n",
       "1    Ag20Bi20CsI60  ['TiO2-c', 'TiO2-mp']  ['PTB7-th']     1.86   \n",
       "2          Ag2BiI5  ['TiO2-c', 'TiO2-mp']     ['PTAA']     2.22   \n",
       "3          Ag3BiI6  ['TiO2-c', 'TiO2-mp']     ['P3HT']     1.80   \n",
       "4          Ag3BiI6  ['TiO2-c', 'TiO2-mp']     ['PTAA']     0.00   \n",
       "\n",
       "                                        device_stack   pce  \\\n",
       "0  ['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...  3.53   \n",
       "1  ['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...  3.53   \n",
       "2  ['SLG', 'ITO', 'TiO2-c', 'TiO2-mp', 'Perovskit...  2.60   \n",
       "3  ['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...  2.32   \n",
       "4  ['SLG', 'FTO', 'TiO2-c', 'TiO2-mp', 'Perovskit...  4.30   \n",
       "\n",
       "             etl_SMILES                                         htl_SMILES  \n",
       "0  [O=[Ti]=O, O=[Ti]=O]                                  [CCCCCCC1=CSC=C1]  \n",
       "1  [O=[Ti]=O, O=[Ti]=O]  [CCC(=CF)COC1=CC=C(C=C1)C23CCC(CC2)(CC3)C(=O)N...  \n",
       "2  [O=[Ti]=O, O=[Ti]=O]    [CC1=CC(=C(C(=C1)C)N(C2=CC=CC=C2)C3=CC=CC=C3)C]  \n",
       "3  [O=[Ti]=O, O=[Ti]=O]                                  [CCCCCCC1=CSC=C1]  \n",
       "4  [O=[Ti]=O, O=[Ti]=O]    [CC1=CC(=C(C(=C1)C)N(C2=CC=CC=C2)C3=CC=CC=C3)C]  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_identified = pd.read_csv('data_RO2/df_ml_ready.csv')\n",
    "\n",
    "df_all_identified['etl_SMILES'] = df_all_identified['etl_SMILES'].apply(ast.literal_eval)\n",
    "df_all_identified['htl_SMILES'] = df_all_identified['htl_SMILES'].apply(ast.literal_eval)\n",
    "\n",
    "df_all_identified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44478171-8347-4953-8049-11711d4f22a2",
   "metadata": {},
   "source": [
    "# Label encoding for model comparison\n",
    "For the comparison of the three final models, the label-encoding of the CTLs would be needed.\n",
    "Therefore, the label-encoding is already done here, even though it will not be used but for that one model.\n",
    "If you want to do the label encoding routine, search for the commented out lines throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c1cc9582-60c4-44ee-b349-aa24b8b33dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the unique values\n",
    "etl_values = df_all_identified['etl'].unique()\n",
    "htl_values = df_all_identified['htl'].unique()\n",
    "\n",
    "# Create a dictionary mapping each value to an integer\n",
    "etl_dict = {value: index for index, value in enumerate(etl_values)}\n",
    "htl_dict = {value: index for index, value in enumerate(htl_values)}\n",
    "\n",
    "# Replace each value with its corresponding integer\n",
    "df_all_identified['etl_encoded'] = df_all_identified['etl'].map(etl_dict)\n",
    "df_all_identified['htl_encoded'] = df_all_identified['htl'].map(htl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c8b5e-215a-4900-b8c2-5307984273b2",
   "metadata": {},
   "source": [
    "# GNN-specific preparation\n",
    "\n",
    "## Define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "476d8005-9443-4ac6-8495-a136b64ec1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of elements to consider\n",
    "ELEMENTS = ['H', 'C', 'N', 'O', 'F', 'P', 'S', 'Cl', 'Br', 'I', 'B', 'Li', 'Na',\n",
    "             'Mg', 'Al', 'Si', 'K', 'Ca', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co',\n",
    "              'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Rb', 'Sr', 'Y', 'Zr', \n",
    "              'Nb', 'Mo', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', \n",
    "              'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', \n",
    "              'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', \n",
    "              'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Th', 'U']\n",
    "\n",
    "# Function to parse chemical formula for absorber\n",
    "def parse_formula(formula):\n",
    "    '''\n",
    "    Parse a chemical formula and return a dictionary of element counts.\n",
    "    '''\n",
    "    element_pattern = r'([A-Z][a-z]?)(\\d*)'\n",
    "    matches = re.findall(element_pattern, formula)\n",
    "    element_counts = {}\n",
    "    for (element, count) in matches:\n",
    "        if element in element_counts:\n",
    "            element_counts[element] += int(count) if count else 1\n",
    "        else:\n",
    "            element_counts[element] = int(count) if count else 1\n",
    "    return element_counts\n",
    "\n",
    "\n",
    "# Function to convert formula to feature vector\n",
    "def formula_to_features(formula):\n",
    "    '''\n",
    "    Convert a chemical formula to a feature vector.\n",
    "    '''\n",
    "    element_counts = parse_formula(formula)\n",
    "    features = torch.zeros(len(ELEMENTS), dtype=torch.float)\n",
    "    for i, element in enumerate(ELEMENTS):\n",
    "        if element in element_counts:\n",
    "            features[i] = element_counts[element]\n",
    "    return features\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    if smiles == \"no_ctl\":\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "# atomic number only    \n",
    "    \n",
    "    # Node features: number of each atom in the molecule\n",
    "    #atom_features = []\n",
    "    #for atom in mol.GetAtoms():\n",
    "    #    atom_features.append([atom.GetAtomicNum()])\n",
    "    #x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "# atomic number and atomic mass     \n",
    "    \n",
    "    # atom_features = []\n",
    "    # for atom in mol.GetAtoms():\n",
    "    #     atomic_num = atom.GetAtomicNum()\n",
    "    #     atomic_weight = atom.GetMass()\n",
    "    #     atom_features.append([atomic_num, atomic_weight])\n",
    "    # x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    \n",
    "# multiple additional features\n",
    "\n",
    "    def get_atom_features(atom):\n",
    "        features = [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetMass(),\n",
    "            atom.GetExplicitValence(),\n",
    "            atom.GetFormalCharge(),\n",
    "            atom.GetHybridization(),\n",
    "            atom.GetIsAromatic(),\n",
    "            atom.GetNumRadicalElectrons(),\n",
    "            atom.GetTotalValence(),\n",
    "            atom.IsInRing()\n",
    "        ]\n",
    "        return [float(f) if not isinstance(f, bool) else int(f) for f in features]\n",
    "\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(get_atom_features(atom))\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    \n",
    "# End of additional features\n",
    "    \n",
    "    # Edge indices\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_index.append([i, j])\n",
    "        edge_index.append([j, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_molecular_data(smiles_list):\n",
    "    '''\n",
    "    Convert a list of SMILES strings to a list of PyTorch Geometric Data objects.\n",
    "    '''\n",
    "    data_list = []\n",
    "    for smiles in smiles_list:\n",
    "        data = smiles_to_graph(smiles)\n",
    "        if data:\n",
    "            data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# For multiple graphs\n",
    "def combine_graphs(graphs):\n",
    "    '''\n",
    "    Combine a list of PyTorch Geometric Data objects into a single object.\n",
    "    '''\n",
    "    if not graphs:\n",
    "        return None\n",
    "\n",
    "    x = torch.cat([g.x for g in graphs], dim=0) # puts the x's together one after the other\n",
    "    edge_index_list = []\n",
    "    offset = 0\n",
    "    for g in graphs:\n",
    "        edge_index_list.append(g.edge_index + offset)\n",
    "        offset += g.num_nodes\n",
    "    edge_index = torch.cat(edge_index_list, dim=1) # puts the edge-indices together\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d897c-92ea-447c-865d-393bf127c9da",
   "metadata": {},
   "source": [
    "## Build graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ebffc2be-7bba-460b-8b1e-57d96fa223dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0\n",
      "Progress: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:42] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2000\n",
      "Progress: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 5000\n"
     ]
    }
   ],
   "source": [
    "molecular_data = []\n",
    "\n",
    "\n",
    "for index, row in df_all_identified.iterrows():\n",
    "    if index % 1000 == 0:  # Print progress every 10 rows\n",
    "        print(f\"Progress: {index}\")\n",
    "\n",
    "    # Check for empty graph lists\n",
    "    etl_graphs = prepare_molecular_data(row['etl_SMILES'])\n",
    "    htl_graphs = prepare_molecular_data(row['htl_SMILES'])\n",
    "\n",
    "    etl_data = combine_graphs(etl_graphs)\n",
    "    htl_data = combine_graphs(htl_graphs)\n",
    "\n",
    "    absorber_features = formula_to_features(row['reduced_formulas'])\n",
    "    pce = torch.tensor([row['pce']], dtype=torch.float)\n",
    "    bandgap =  torch.tensor([row['bandgap']], dtype=torch.float)\n",
    "    # etl_encoded = torch.tensor([row['etl_encoded']], dtype=torch.float) # for CTL label encoding model\n",
    "    # htl_encoded = torch.tensor([row['htl_encoded']], dtype=torch.float) # for CTL label encoding model\n",
    "\n",
    "    molecular_data.append({\n",
    "        'etl': etl_data if etl_graphs else None,  # Assign None if etl_graphs is empty\n",
    "        'htl': htl_data if htl_graphs else None,  # Assign None if htl_graphs is empty\n",
    "        'absorber': absorber_features,\n",
    "        'pce': pce,\n",
    "        'bandgap': bandgap,\n",
    "        # 'index': index # this was originally used to ensure shuffling is the same in all models\n",
    "        # 'etl_encoded': etl_encoded, # for CTL label encoding model\n",
    "        # 'htl_encoded': htl_encoded  # for CTL label encoding model\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b4cf2937-b627-40e2-9a16-b40fffd4d7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensure_2d(tensor):\n",
    "    if tensor.dim() < 1:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() < 2:\n",
    "        tensor = tensor.unsqueeze(0)  # Add a dimension at the beginning\n",
    "    return tensor\n",
    "\n",
    "# Define a class for MolecularDataset\n",
    "class MolecularDataset(Dataset):\n",
    "    '''\n",
    "    A PyTorch Dataset class for loading the molecular data.\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        \n",
    "        layers = ['etl', 'htl']\n",
    "        layer_data = {}\n",
    "\n",
    "        # default_data = {'x': torch.zeros((1,1),  dtype=torch.float), 'edge_index': torch.zeros((2,1), dtype=torch.long)}\n",
    "        default_data = {'x': torch.zeros((1,9),  dtype=torch.float), 'edge_index': torch.zeros((2,1), dtype=torch.long)} # currently diff\n",
    "        \n",
    "        for layer in layers:\n",
    "            if data_point[layer] is not None:  # Check if layer key exists\n",
    "                x_data = data_point[layer]['x']\n",
    "                edge_index_data = data_point[layer]['edge_index']\n",
    "                \n",
    "                x_data = ensure_2d(x_data)\n",
    "                edge_index_data = ensure_2d(edge_index_data)\n",
    "                if torch.numel(edge_index_data) == 0:\n",
    "                    edge_index_data = torch.zeros((2,0), dtype=torch.long)\n",
    "                \n",
    "                layer_data[layer] = {\n",
    "                    'x': x_data,\n",
    "                    'edge_index': edge_index_data,\n",
    "                }\n",
    "            else:\n",
    "                layer_data[layer] = default_data\n",
    "        \n",
    "        absorber_features = data_point['absorber']\n",
    "        pce = data_point['pce']\n",
    "        bandgap = data_point['bandgap']\n",
    "        # etl_encoded = data_point['etl_encoded'] # only for label encoding\n",
    "        # htl_encoded = data_point['htl_encoded'] # only for label encoding\n",
    "\n",
    "        etl =layer_data['etl']['x']\n",
    "        htl =layer_data['htl']['x']\n",
    "        etl_edge_index=layer_data['etl']['edge_index']\n",
    "        htl_edge_index=layer_data['htl']['edge_index']\n",
    "                \n",
    "        return {\n",
    "            'etl_features':etl,\n",
    "            'htl_features':htl,\n",
    "            'etl_edge_indices': etl_edge_index,\n",
    "            'htl_edge_indices': htl_edge_index,\n",
    "            'absorber': absorber_features,\n",
    "            'pce': pce, \n",
    "            'bandgap':bandgap,\n",
    "            # 'etl_encoded': etl_encoded, # for CTL label encoding model\n",
    "            # 'htl_encoded': htl_encoded,  # for CTL label encoding model\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f1d13-6e93-4840-b55c-b94bf92f6b26",
   "metadata": {},
   "source": [
    "# Train Test Split and Dataloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "640af6d0-3bda-4a53-98be-2126608f3f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train/test/val split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate sizes based on ratios\n",
    "total_size = len(molecular_data)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# shuffle the data to unbias-the val-test-train-split\n",
    "rng = np.random.default_rng(seed=42)\n",
    "rng.shuffle(molecular_data)\n",
    "\n",
    "# Split the data\n",
    "train_data = molecular_data[:train_size]\n",
    "val_data = molecular_data[train_size:train_size + val_size]\n",
    "test_data = molecular_data[train_size + val_size:]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(MolecularDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(MolecularDataset(val_data), batch_size=batch_size)\n",
    "test_loader = DataLoader(MolecularDataset(test_data), batch_size=batch_size)\n",
    "\n",
    "single_data = molecular_data[:2]\n",
    "# single_data = [molecular_data[0], molecular_data[2]]\n",
    "\n",
    "single_loader = DataLoader(MolecularDataset(single_data), batch_size=batch_size, shuffle=True)\n",
    "single_data = molecular_data[:-2]\n",
    "single_val_loader = DataLoader(MolecularDataset(single_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fdbfb-814d-4a20-9677-d651fcfeaa20",
   "metadata": {},
   "source": [
    "# Export the random shuffling to use the same test set for CrabNet and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0f14e6f7-4520-4424-b35a-b9844ddecbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for entry in molecular_data:\n",
    "    index_list.append(entry['index'])\n",
    "\n",
    "with open(\"data_RO2/index_for_train_test_split.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd7f1a-9726-4410-9f20-64874a23517c",
   "metadata": {},
   "source": [
    "# GNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41ee9458-91e8-445c-8d56-bf091cb7aee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MPNN, self).__init__(aggr='add')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.lin(aggr_out)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32\n",
    "        # self.absorber_embedding_dimensions = 16\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        # arbitrary choice:\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        # in_channels are describing the number of node features, atom-type, weight, polarity...\n",
    "        # self.etl_mpnn = MPNN(in_channels=1, out_channels=self.etl_embedding_dimensions)\n",
    "        # self.htl_mpnn = MPNN(in_channels=1, out_channels=self.htl_embedding_dimensions)\n",
    "        self.etl_mpnn = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions) # input channels changed to 2 for now\n",
    "        self.htl_mpnn = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions) # input channels changed to 2 for now\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension, self.hidden_dimension)\n",
    "        \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear( self.hidden_dimension,  self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "        # 1, because we just want to predict pce:\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "        \n",
    "        # self.fc_absorber = torch.nn.Linear(self.absorber_dimensions, self.absorber_embedding_dimensions)\n",
    "\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):\n",
    "        etl_x = self.etl_mpnn(etl_features, etl_edge_indices) # comment this out for no-graph prediction\n",
    "        # etl_x = self.etl_mpnn(etl_x, etl_edge_indices)\n",
    "        etl_x = global_mean_pool(etl_x, torch.zeros(etl_x.size(0), dtype=torch.long)) # comment this out for no-graph prediction\n",
    "        \n",
    "        htl_x = self.htl_mpnn(htl_features, htl_edge_indices) # comment this out for no-graph prediction\n",
    "        # htl_x = self.htl_mpnn(htl_x, htl_edge_indices)\n",
    "        htl_x = global_mean_pool(htl_x, torch.zeros(htl_x.size(0), dtype=torch.long)) # comment this out for no-graph prediction\n",
    "        \n",
    "        # etl_x = torch.zeros([1,self.etl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        # htl_x = torch.zeros([1,self.htl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "\n",
    "        # absorbers_embed = self.fc_absorber(absorbers)\n",
    "        # x = torch.cat([etl_x, htl_x, absorbers_embed], dim=1) \n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        x = F.softplus(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cf7ee-1bee-42f8-9aeb-d1d52290470a",
   "metadata": {},
   "source": [
    "# Model code for 2 MP layer Net (used after GNN_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74d50dc8-e84f-4ca5-9360-58eae948eb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        # Two GCN layers for ETL processing\n",
    "        self.etl_mpnn1 = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions)\n",
    "        self.etl_mpnn2 = GCNConv(in_channels=self.etl_embedding_dimensions, out_channels=self.etl_embedding_dimensions)\n",
    "\n",
    "        # Two GCN layers for HTL processing\n",
    "        self.htl_mpnn1 = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions)\n",
    "        self.htl_mpnn2 = GCNConv(in_channels=self.htl_embedding_dimensions, out_channels=self.htl_embedding_dimensions)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension, self.hidden_dimension)\n",
    "        \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear(self.hidden_dimension, self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):\n",
    "        # Two-layer processing for ETL\n",
    "        etl_x = self.etl_mpnn1(etl_features, etl_edge_indices)\n",
    "        etl_x = F.relu(etl_x)  # Add activation after the first layer\n",
    "        etl_x = self.etl_mpnn2(etl_x, etl_edge_indices)\n",
    "        etl_x = global_mean_pool(etl_x, torch.zeros(etl_x.size(0), dtype=torch.long))\n",
    "    \n",
    "        # Two-layer processing for HTL\n",
    "        htl_x = self.htl_mpnn1(htl_features, htl_edge_indices)\n",
    "        htl_x = F.relu(htl_x)  # Add activation after the first layer\n",
    "        htl_x = self.htl_mpnn2(htl_x, htl_edge_indices)\n",
    "        htl_x = global_mean_pool(htl_x, torch.zeros(htl_x.size(0), dtype=torch.long))\n",
    "    \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers:\n",
    "            x = F.leaky_relu(layer(x))\n",
    "    \n",
    "        x = self.fc_out(x)\n",
    "        # x = F.softplus(x) # Model performs better without softplus layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29f839-0d3a-48de-886e-49493b4423b9",
   "metadata": {},
   "source": [
    "# Model code for label-encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9a25f2cc-b725-4c15-aad8-183884dfda03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        self.etl_label_dimension = 1 # only for label_encoding model\n",
    "        self.htl_label_dimension = 1 # only for label_encoding model\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        # Two GCN layers for ETL processing\n",
    "        self.etl_mpnn1 = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions)\n",
    "        self.etl_mpnn2 = GCNConv(in_channels=self.etl_embedding_dimensions, out_channels=self.etl_embedding_dimensions)\n",
    "\n",
    "        # Two GCN layers for HTL processing\n",
    "        self.htl_mpnn1 = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions)\n",
    "        self.htl_mpnn2 = GCNConv(in_channels=self.htl_embedding_dimensions, out_channels=self.htl_embedding_dimensions)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension + self.etl_label_dimension + self.htl_label_dimension, self.hidden_dimension)\n",
    "        \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear(self.hidden_dimension, self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "\n",
    "\n",
    "    # Forward function for label encoding model \n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap, etl_encoded, htl_encoded):        \n",
    "        etl_x = torch.zeros([1,self.etl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        htl_x = torch.zeros([1,self.htl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap, etl_encoded, htl_encoded], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3050b-5439-4a59-82b5-a4a521f3493d",
   "metadata": {},
   "source": [
    "# Model code for absorber only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f40d380-dd1c-4ebe-861c-0872f5e64672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        # Two GCN layers for ETL processing\n",
    "        self.etl_mpnn1 = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions)\n",
    "        self.etl_mpnn2 = GCNConv(in_channels=self.etl_embedding_dimensions, out_channels=self.etl_embedding_dimensions)\n",
    "\n",
    "        # Two GCN layers for HTL processing\n",
    "        self.htl_mpnn1 = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions)\n",
    "        self.htl_mpnn2 = GCNConv(in_channels=self.htl_embedding_dimensions, out_channels=self.htl_embedding_dimensions)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension, self.hidden_dimension)\n",
    "        \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear(self.hidden_dimension, self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "\n",
    "\n",
    "    # Forward function for label encoding model \n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):        \n",
    "        etl_x = torch.zeros([1,self.etl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        htl_x = torch.zeros([1,self.htl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296efad3-1024-4c0c-9d87-fa0807f60d2e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e86bec95-d9fb-4028-a662-28d071c86a06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "(new best)\n",
      "Epoch 1, Loss: 233.53771056115667, Val-Loss: 209.71259878746258\n",
      "(new best)\n",
      "Epoch 2, Loss: 185.04073529751977, Val-Loss: 170.2943413979013\n",
      "(new best)\n",
      "Epoch 3, Loss: 176.54816320003735, Val-Loss: 161.10710406402382\n",
      "(new best)\n",
      "Epoch 4, Loss: 175.08815293748594, Val-Loss: 158.65556909545836\n",
      "(new best)\n",
      "Epoch 5, Loss: 173.48446267847422, Val-Loss: 157.25966958454174\n",
      "(new best)\n",
      "Epoch 6, Loss: 171.31739657502376, Val-Loss: 156.17577319105612\n",
      "(new best)\n",
      "Epoch 7, Loss: 169.6154137709328, Val-Loss: 154.53592303191962\n",
      "(new best)\n",
      "Epoch 8, Loss: 167.5090802750294, Val-Loss: 153.4994418182826\n",
      "(new best)\n",
      "Epoch 9, Loss: 165.5528997163926, Val-Loss: 153.31697693772313\n",
      "(new best)\n",
      "Epoch 10, Loss: 164.22748510599064, Val-Loss: 150.8874346101416\n",
      "(new best)\n",
      "Epoch 11, Loss: 162.24106735755822, Val-Loss: 149.5451978576813\n",
      "(new best)\n",
      "Epoch 12, Loss: 160.49956948638606, Val-Loss: 147.84836312765566\n",
      "Epoch 13, Loss: 157.99387671316956, Val-Loss: 148.5917103130824\n",
      "(new best)\n",
      "Epoch 14, Loss: 157.2150200907189, Val-Loss: 144.00019579861876\n",
      "(new best)\n",
      "Epoch 15, Loss: 154.27753710055399, Val-Loss: 142.82162099768755\n",
      "(new best)\n",
      "Epoch 16, Loss: 152.5619494142896, Val-Loss: 140.7512583379046\n",
      "(new best)\n",
      "Epoch 17, Loss: 149.15532561180987, Val-Loss: 139.73936203996413\n",
      "(new best)\n",
      "Epoch 18, Loss: 146.18197418078282, Val-Loss: 135.47472582629632\n",
      "(new best)\n",
      "Epoch 19, Loss: 141.62850576637913, Val-Loss: 134.56902634280965\n",
      "(new best)\n",
      "Epoch 20, Loss: 139.08419538229683, Val-Loss: 129.85797277664958\n",
      "Epoch 21, Loss: 134.90312813787813, Val-Loss: 130.05270777868603\n",
      "(new best)\n",
      "Epoch 22, Loss: 130.78439107783475, Val-Loss: 118.7100366373848\n",
      "(new best)\n",
      "Epoch 23, Loss: 122.06170778046837, Val-Loss: 113.20586390621771\n",
      "(new best)\n",
      "Epoch 24, Loss: 115.55306076954206, Val-Loss: 105.00236007044109\n",
      "(new best)\n",
      "Epoch 25, Loss: 108.81821936209344, Val-Loss: 96.43058061318013\n",
      "(new best)\n",
      "Epoch 26, Loss: 99.94942004122615, Val-Loss: 86.64023655809366\n",
      "(new best)\n",
      "Epoch 27, Loss: 91.36380595981339, Val-Loss: 81.16080766602276\n",
      "(new best)\n",
      "Epoch 28, Loss: 83.37189664374, Val-Loss: 72.2734198596117\n",
      "(new best)\n",
      "Epoch 29, Loss: 77.13362273733135, Val-Loss: 64.3067875325537\n",
      "Epoch 30, Loss: 68.45404115337378, Val-Loss: 64.45260589032804\n",
      "(new best)\n",
      "Epoch 31, Loss: 64.54327648813862, Val-Loss: 51.28621473189259\n",
      "Epoch 32, Loss: 59.09342192618556, Val-Loss: 53.99747379396036\n",
      "(new best)\n",
      "Epoch 33, Loss: 55.93668961850147, Val-Loss: 46.76508626982942\n",
      "(new best)\n",
      "Epoch 34, Loss: 48.68533121916784, Val-Loss: 41.11366282526972\n",
      "Epoch 35, Loss: 47.98428042433868, Val-Loss: 55.147260549351564\n",
      "Epoch 36, Loss: 43.6329662580908, Val-Loss: 54.4564286661416\n",
      "Epoch 37, Loss: 46.889680539302795, Val-Loss: 66.1976862242935\n",
      "(new best)\n",
      "Epoch 38, Loss: 40.487928297353974, Val-Loss: 38.90643822115702\n",
      "(new best)\n",
      "Epoch 39, Loss: 46.08870698782173, Val-Loss: 32.45266489305598\n",
      "Epoch 40, Loss: 41.30723689762644, Val-Loss: 39.22664581034058\n",
      "Epoch 41, Loss: 36.82884117203071, Val-Loss: 34.9725679089529\n",
      "(new best)\n",
      "Epoch 42, Loss: 44.58014726342543, Val-Loss: 32.18443979373131\n",
      "(new best)\n",
      "Epoch 43, Loss: 52.19967811939615, Val-Loss: 30.126372240303585\n",
      "(new best)\n",
      "Epoch 44, Loss: 31.438495325675873, Val-Loss: 28.699620415495254\n",
      "Epoch 45, Loss: 41.00479852589676, Val-Loss: 31.540776541677637\n",
      "Epoch 46, Loss: 35.7756371184215, Val-Loss: 105.99436957788802\n",
      "Epoch 47, Loss: 35.38302031371369, Val-Loss: 38.682084805223845\n",
      "Epoch 48, Loss: 38.31939253697049, Val-Loss: 55.035451996856075\n",
      "Epoch 49, Loss: 32.94314568329324, Val-Loss: 30.813777781319896\n",
      "Epoch 50, Loss: 31.061632105062614, Val-Loss: 33.049670962671364\n",
      "Epoch 51, Loss: 35.976058463134585, Val-Loss: 34.11616184509365\n",
      "Epoch 52, Loss: 35.69913857099428, Val-Loss: 32.559722377107406\n",
      "Epoch 53, Loss: 28.56125891191424, Val-Loss: 47.83962837143697\n",
      "Epoch 54, Loss: 36.67346113037047, Val-Loss: 29.12890870292238\n",
      "Epoch 55, Loss: 38.59592561575341, Val-Loss: 142.631846395776\n",
      "(new best)\n",
      "Epoch 56, Loss: 28.174175522833526, Val-Loss: 22.899301975387452\n",
      "Epoch 57, Loss: 31.560837396337337, Val-Loss: 30.237315856244912\n",
      "Epoch 58, Loss: 28.507982122634257, Val-Loss: 30.44402724697117\n",
      "Epoch 59, Loss: 35.97868364523055, Val-Loss: 26.1620008649027\n",
      "Epoch 60, Loss: 26.97400817281078, Val-Loss: 34.81175608197519\n",
      "Epoch 61, Loss: 29.8774790581412, Val-Loss: 76.13869006014066\n",
      "(new best)\n",
      "Epoch 62, Loss: 29.908750081516608, Val-Loss: 21.893118599823925\n",
      "(new best)\n",
      "Epoch 63, Loss: 31.626767082549712, Val-Loss: 20.6777794058381\n",
      "Epoch 64, Loss: 30.198021865782444, Val-Loss: 27.5653953306352\n",
      "Epoch 65, Loss: 26.130365567864473, Val-Loss: 25.920449947784594\n",
      "Epoch 66, Loss: 29.209201451692778, Val-Loss: 71.90220306722848\n",
      "Epoch 67, Loss: 27.06616288079657, Val-Loss: 24.42322776542004\n",
      "Epoch 68, Loss: 25.43941748657309, Val-Loss: 51.98358830740086\n",
      "Epoch 69, Loss: 26.24988142875705, Val-Loss: 39.266942010324264\n",
      "Epoch 70, Loss: 29.33297908356924, Val-Loss: 23.736105806664686\n",
      "Epoch 71, Loss: 20.24824379661965, Val-Loss: 29.116846564499802\n",
      "Epoch 72, Loss: 28.999748919607075, Val-Loss: 22.851085804329767\n",
      "Epoch 73, Loss: 22.249448017178093, Val-Loss: 106.87826638212447\n",
      "(new best)\n",
      "Epoch 74, Loss: 23.595242189907, Val-Loss: 20.43749135272574\n",
      "Epoch 75, Loss: 21.780261242361185, Val-Loss: 25.416944916527413\n",
      "Epoch 76, Loss: 23.150208528107562, Val-Loss: 25.765652649135443\n",
      "Epoch 77, Loss: 21.567362980987568, Val-Loss: 25.272586806680234\n",
      "(new best)\n",
      "Epoch 78, Loss: 21.897726641233447, Val-Loss: 19.46751464692214\n",
      "Epoch 79, Loss: 21.728448643027715, Val-Loss: 30.1322256967992\n",
      "Epoch 80, Loss: 21.89096106743111, Val-Loss: 26.613528936543734\n",
      "Epoch 81, Loss: 22.91516380906171, Val-Loss: 22.359456164808666\n",
      "Epoch 82, Loss: 24.378239523912494, Val-Loss: 27.120465009290808\n",
      "Epoch 83, Loss: 22.505000525037758, Val-Loss: 19.81421365042051\n",
      "(new best)\n",
      "Epoch 84, Loss: 20.838904340276073, Val-Loss: 18.83410620674808\n",
      "(new best)\n",
      "Epoch 85, Loss: 21.70963185642849, Val-Loss: 18.800527142740176\n",
      "Epoch 86, Loss: 29.567382928813924, Val-Loss: 34.993362034592124\n",
      "Epoch 87, Loss: 21.500776915090395, Val-Loss: 24.492175091695536\n",
      "Epoch 88, Loss: 31.279674176967557, Val-Loss: 20.647294300506633\n",
      "Epoch 89, Loss: 18.717958757639774, Val-Loss: 19.053041304748938\n",
      "(new best)\n",
      "Epoch 90, Loss: 21.289498018412473, Val-Loss: 18.637127930744885\n",
      "Epoch 91, Loss: 21.068183888928157, Val-Loss: 18.74965775904382\n",
      "Epoch 92, Loss: 20.94107021658228, Val-Loss: 24.369943854016647\n",
      "Epoch 93, Loss: 20.68432398933378, Val-Loss: 19.20789679885245\n",
      "Epoch 94, Loss: 20.330068821196708, Val-Loss: 21.543093614276035\n",
      "Epoch 95, Loss: 18.721121852344563, Val-Loss: 21.207747866991195\n",
      "(new best)\n",
      "Epoch 96, Loss: 22.53460076293054, Val-Loss: 18.50404410763687\n",
      "Epoch 97, Loss: 23.406281543859333, Val-Loss: 25.099937390134645\n",
      "Epoch 98, Loss: 24.541269242030708, Val-Loss: 20.444180617863367\n",
      "(new best)\n",
      "Epoch 99, Loss: 21.515493247815424, Val-Loss: 17.653715358095436\n",
      "Epoch 100, Loss: 19.806207978180534, Val-Loss: 18.070943212979383\n",
      "Epoch 101, Loss: 21.190060419499332, Val-Loss: 18.47400388872258\n",
      "Epoch 102, Loss: 18.50538081197856, Val-Loss: 26.727471983827332\n",
      "Epoch 103, Loss: 19.38176466626095, Val-Loss: 19.90757423740928\n",
      "Epoch 104, Loss: 24.01316851344195, Val-Loss: 27.813684402653568\n",
      "Epoch 105, Loss: 23.6917293627997, Val-Loss: 22.34589152570135\n",
      "Epoch 106, Loss: 20.985730835874183, Val-Loss: 42.53506091942244\n",
      "Epoch 107, Loss: 20.62204686047654, Val-Loss: 19.704059257968147\n",
      "Epoch 108, Loss: 20.141931678372973, Val-Loss: 17.970399235206088\n",
      "(new best)\n",
      "Epoch 109, Loss: 19.091264526169375, Val-Loss: 17.14403848543034\n",
      "(new best)\n",
      "Epoch 110, Loss: 19.11412262449445, Val-Loss: 17.121255415935995\n",
      "Epoch 111, Loss: 18.944533342263025, Val-Loss: 18.99511189393452\n",
      "Epoch 112, Loss: 20.67913482770771, Val-Loss: 19.047371378071446\n",
      "Epoch 113, Loss: 18.233533174226036, Val-Loss: 18.715917210706348\n",
      "Epoch 114, Loss: 18.20114297362096, Val-Loss: 20.720653438361428\n",
      "Epoch 115, Loss: 18.79966004595163, Val-Loss: 18.30287161697114\n",
      "(new best)\n",
      "Epoch 116, Loss: 18.8052112956587, Val-Loss: 16.842925691210027\n",
      "Epoch 117, Loss: 19.215862861830505, Val-Loss: 46.55699667546853\n",
      "Epoch 118, Loss: 18.93202360556416, Val-Loss: 18.837921488002046\n",
      "Epoch 119, Loss: 18.286486331176665, Val-Loss: 20.69727986626105\n",
      "Epoch 120, Loss: 18.397065918025884, Val-Loss: 17.899547520479487\n",
      "Epoch 121, Loss: 18.47938294305506, Val-Loss: 20.574036364498788\n",
      "Epoch 122, Loss: 18.48761065864088, Val-Loss: 17.308184878428836\n",
      "Epoch 123, Loss: 19.070369661966563, Val-Loss: 17.257226515749522\n",
      "Epoch 124, Loss: 17.847823743608043, Val-Loss: 17.16882412489481\n",
      "(new best)\n",
      "Epoch 125, Loss: 18.011220682272594, Val-Loss: 16.584673152556462\n",
      "Epoch 126, Loss: 20.00701110229431, Val-Loss: 19.707973391358507\n",
      "Epoch 127, Loss: 19.24868209475882, Val-Loss: 18.665676109540488\n",
      "Epoch 128, Loss: 19.123488167935747, Val-Loss: 22.58574419538265\n",
      "Epoch 129, Loss: 18.563678732564767, Val-Loss: 17.823550872934423\n",
      "Epoch 130, Loss: 18.278044770062177, Val-Loss: 23.23463812483667\n",
      "Epoch 131, Loss: 18.14506140446254, Val-Loss: 17.060156052719186\n",
      "Epoch 132, Loss: 17.331255718670416, Val-Loss: 21.73757843509249\n",
      "Epoch 133, Loss: 17.474773309939604, Val-Loss: 18.65427160210237\n",
      "(new best)\n",
      "Epoch 134, Loss: 16.794663043702837, Val-Loss: 16.535169649361336\n",
      "(new best)\n",
      "Epoch 135, Loss: 17.545853974365837, Val-Loss: 16.082161897706087\n",
      "Epoch 136, Loss: 18.229300352363502, Val-Loss: 17.293526071702793\n",
      "Epoch 137, Loss: 16.751610268434288, Val-Loss: 16.56335906772176\n",
      "Epoch 138, Loss: 16.751598306835746, Val-Loss: 17.649057859675825\n",
      "(new best)\n",
      "Epoch 139, Loss: 17.24423281463121, Val-Loss: 15.944949309657106\n",
      "Epoch 140, Loss: 17.439830097672175, Val-Loss: 16.942690743282657\n",
      "Epoch 141, Loss: 16.59848927396989, Val-Loss: 22.240813983208565\n",
      "Epoch 142, Loss: 17.972429273348272, Val-Loss: 16.68194804641584\n",
      "(new best)\n",
      "Epoch 143, Loss: 16.11952098857029, Val-Loss: 15.609656184270463\n",
      "Epoch 144, Loss: 17.901589868645303, Val-Loss: 17.67334102423487\n",
      "Epoch 145, Loss: 16.628752512904878, Val-Loss: 17.778844647321396\n",
      "Epoch 146, Loss: 16.724257093124574, Val-Loss: 16.207990778302964\n",
      "Epoch 147, Loss: 17.11483771935951, Val-Loss: 16.00653876615334\n",
      "Epoch 148, Loss: 17.07824515920369, Val-Loss: 16.244311386770384\n",
      "Epoch 149, Loss: 17.2218902103076, Val-Loss: 31.970675302040345\n",
      "Epoch 150, Loss: 16.755214587966087, Val-Loss: 15.813103341398651\n",
      "Epoch 151, Loss: 16.31039179886102, Val-Loss: 17.39020996721615\n",
      "Epoch 152, Loss: 34.649147297642614, Val-Loss: 77.11145567038254\n",
      "(new best)\n",
      "Epoch 153, Loss: 21.025272818635163, Val-Loss: 15.536017739583224\n",
      "Epoch 154, Loss: 15.792851867456987, Val-Loss: 18.26108009740302\n",
      "Epoch 155, Loss: 16.45723899264153, Val-Loss: 15.991523546856108\n",
      "Epoch 156, Loss: 17.12678695995866, Val-Loss: 18.587903102337275\n",
      "Epoch 157, Loss: 16.72471201030209, Val-Loss: 15.582521060552843\n",
      "Epoch 158, Loss: 17.43841815026907, Val-Loss: 15.547647381808835\n",
      "Epoch 159, Loss: 16.188463868399907, Val-Loss: 15.954723580348295\n",
      "Epoch 160, Loss: 16.74887563960016, Val-Loss: 21.31533077542331\n",
      "(new best)\n",
      "Epoch 161, Loss: 16.504567492944425, Val-Loss: 15.307897693076347\n",
      "Epoch 162, Loss: 17.14755808563753, Val-Loss: 17.229976700542238\n",
      "Epoch 163, Loss: 16.010866597312113, Val-Loss: 26.03279341114013\n",
      "Epoch 164, Loss: 16.941265733355497, Val-Loss: 16.162128443242995\n",
      "Epoch 165, Loss: 15.836962034562426, Val-Loss: 16.242332377926232\n",
      "Epoch 166, Loss: 16.383153500646493, Val-Loss: 15.814209833439518\n",
      "Epoch 167, Loss: 15.823475325843727, Val-Loss: 15.391385960911435\n",
      "Epoch 168, Loss: 16.162200724330177, Val-Loss: 17.353174284511663\n",
      "Epoch 169, Loss: 16.410922930503634, Val-Loss: 16.188594476151295\n",
      "(new best)\n",
      "Epoch 170, Loss: 16.005152031773704, Val-Loss: 15.191197194002463\n",
      "Epoch 171, Loss: 15.76131829439743, Val-Loss: 20.498345600685816\n",
      "Epoch 172, Loss: 17.545610408498415, Val-Loss: 15.386305598812728\n",
      "Epoch 173, Loss: 20.271400642531795, Val-Loss: 20.76726883258174\n",
      "Epoch 174, Loss: 17.438585875655196, Val-Loss: 16.062744626541292\n",
      "Epoch 175, Loss: 16.189630975804146, Val-Loss: 18.80176253860756\n",
      "Epoch 176, Loss: 16.137947114629146, Val-Loss: 15.473002320535878\n",
      "Epoch 177, Loss: 15.423184157884132, Val-Loss: 15.520451739349305\n",
      "Epoch 178, Loss: 15.46678840202833, Val-Loss: 16.24998847538789\n",
      "Epoch 179, Loss: 15.867473188227418, Val-Loss: 15.883938140082616\n",
      "(new best)\n",
      "Epoch 180, Loss: 15.898689811837684, Val-Loss: 15.159230109434338\n",
      "Epoch 181, Loss: 16.11550770845696, Val-Loss: 15.231376902144369\n",
      "Epoch 182, Loss: 15.864605775577045, Val-Loss: 17.74231850772714\n",
      "(new best)\n",
      "Epoch 183, Loss: 15.477684712303152, Val-Loss: 14.900234360736182\n",
      "Epoch 184, Loss: 15.254999625382798, Val-Loss: 15.320767083946686\n",
      "Epoch 185, Loss: 15.978356564420631, Val-Loss: 15.00405160291899\n",
      "Epoch 186, Loss: 15.23852169004802, Val-Loss: 17.99933331046419\n",
      "Epoch 187, Loss: 15.080785066280155, Val-Loss: 15.360195012258444\n",
      "Epoch 188, Loss: 15.415094431397304, Val-Loss: 15.612100292918255\n",
      "Epoch 189, Loss: 16.295042215153845, Val-Loss: 15.58627087122397\n",
      "Epoch 190, Loss: 15.154502084071751, Val-Loss: 14.935664519289412\n",
      "Epoch 191, Loss: 15.659828282203684, Val-Loss: 16.491504053453138\n",
      "Epoch 192, Loss: 15.246184711297893, Val-Loss: 16.32214677086256\n",
      "Epoch 193, Loss: 18.926623899130046, Val-Loss: 17.33614202512836\n",
      "Epoch 194, Loss: 15.508867471257112, Val-Loss: 15.81215944523409\n",
      "Epoch 195, Loss: 15.042530641819583, Val-Loss: 16.761995075041998\n",
      "(new best)\n",
      "Epoch 196, Loss: 16.470849336956455, Val-Loss: 14.896948531846903\n",
      "Epoch 197, Loss: 15.402769612275447, Val-Loss: 15.182671090678635\n",
      "Epoch 198, Loss: 14.84440793269434, Val-Loss: 15.283071582294838\n",
      "Epoch 199, Loss: 15.085901698079775, Val-Loss: 16.181148630906126\n",
      "Epoch 200, Loss: 15.083058325980613, Val-Loss: 15.069403392957838\n",
      "Epoch 201, Loss: 15.081939767069798, Val-Loss: 19.71979202904963\n",
      "Epoch 202, Loss: 21.21108294464352, Val-Loss: 15.39235971858878\n",
      "Epoch 203, Loss: 14.719368031436156, Val-Loss: 14.972013880154462\n",
      "Epoch 204, Loss: 15.19388150410704, Val-Loss: 16.076742582984213\n",
      "Epoch 205, Loss: 15.656406033536614, Val-Loss: 16.335099210979838\n",
      "Epoch 206, Loss: 14.99976787569967, Val-Loss: 15.93881447017864\n",
      "Epoch 207, Loss: 15.242766013231565, Val-Loss: 15.557970197531183\n",
      "Epoch 208, Loss: 15.30278446304111, Val-Loss: 15.919652532553194\n",
      "Epoch 209, Loss: 14.960280048670924, Val-Loss: 16.65840809191474\n",
      "Epoch 210, Loss: 15.048661343695867, Val-Loss: 15.081601836053641\n",
      "Epoch 211, Loss: 15.244992560149003, Val-Loss: 15.313989860555965\n",
      "Epoch 212, Loss: 14.68956332838858, Val-Loss: 15.748278672384242\n",
      "(new best)\n",
      "Epoch 213, Loss: 20.050429897596704, Val-Loss: 14.624881185402582\n",
      "Epoch 214, Loss: 14.642369137447849, Val-Loss: 14.796633109778774\n",
      "Epoch 215, Loss: 14.807031524626211, Val-Loss: 16.10823120940256\n",
      "Epoch 216, Loss: 14.831717826144597, Val-Loss: 14.951590212068444\n",
      "Epoch 217, Loss: 14.834121768968169, Val-Loss: 14.913726900425088\n",
      "Epoch 218, Loss: 15.137075986366115, Val-Loss: 15.240086095569536\n",
      "Epoch 219, Loss: 14.665899572133037, Val-Loss: 15.790370988343392\n",
      "Epoch 220, Loss: 15.09392741454978, Val-Loss: 15.795056239547542\n",
      "Epoch 221, Loss: 15.135855595727376, Val-Loss: 15.11319829966919\n",
      "Epoch 222, Loss: 14.676908653282798, Val-Loss: 15.180155414628667\n",
      "Epoch 223, Loss: 14.860972831450598, Val-Loss: 16.1456558094092\n",
      "Epoch 224, Loss: 16.18439682316071, Val-Loss: 15.492252447279528\n",
      "Epoch 225, Loss: 15.057708841644478, Val-Loss: 16.62059031624171\n",
      "Epoch 226, Loss: 14.688927882413834, Val-Loss: 14.812413348896023\n",
      "Epoch 227, Loss: 15.38791327929182, Val-Loss: 15.684277975540821\n",
      "Epoch 228, Loss: 14.663697666680989, Val-Loss: 15.242602824966733\n",
      "Epoch 229, Loss: 14.686914881452958, Val-Loss: 16.01246823752721\n",
      "Epoch 230, Loss: 14.83018839655953, Val-Loss: 15.099719256807774\n",
      "Epoch 231, Loss: 14.515341418228214, Val-Loss: 15.533445169641018\n",
      "Epoch 232, Loss: 14.68014367382088, Val-Loss: 15.231381414376965\n",
      "Epoch 233, Loss: 14.628376929012264, Val-Loss: 16.360515884268967\n",
      "Epoch 234, Loss: 14.561419814890959, Val-Loss: 16.096671100124215\n",
      "Epoch 235, Loss: 14.47785962568841, Val-Loss: 17.931741346881125\n",
      "Epoch 236, Loss: 14.609308776618349, Val-Loss: 15.673791731163881\n",
      "Epoch 237, Loss: 14.546369451451184, Val-Loss: 14.823379217772802\n",
      "Epoch 238, Loss: 14.501179530021828, Val-Loss: 15.186543793619629\n",
      "Epoch 239, Loss: 14.609704754160687, Val-Loss: 14.759823288436568\n",
      "Epoch 240, Loss: 14.718629733011902, Val-Loss: 14.850330047779154\n",
      "Epoch 241, Loss: 14.520886298514467, Val-Loss: 15.115400239037378\n",
      "Epoch 242, Loss: 14.786245684288298, Val-Loss: 17.5327235780715\n",
      "Epoch 243, Loss: 14.670580668181517, Val-Loss: 14.929025486319556\n",
      "Epoch 244, Loss: 14.411801696248277, Val-Loss: 15.700009311021644\n",
      "Epoch 245, Loss: 14.409271316034017, Val-Loss: 15.315804356851528\n",
      "Epoch 246, Loss: 14.733298565881771, Val-Loss: 15.105947792181674\n",
      "Epoch 247, Loss: 14.393593827165486, Val-Loss: 14.847448204003694\n",
      "Epoch 248, Loss: 14.440694665025024, Val-Loss: 15.71755851032004\n",
      "Epoch 249, Loss: 14.519389782799854, Val-Loss: 15.061588665992122\n",
      "Epoch 250, Loss: 14.546472099371913, Val-Loss: 15.021964641955567\n",
      "Epoch 251, Loss: 14.564427690627157, Val-Loss: 15.240423603254849\n",
      "Epoch 252, Loss: 15.654060596672894, Val-Loss: 14.864351761426624\n",
      "Epoch 253, Loss: 14.228061115400173, Val-Loss: 14.793235010329967\n",
      "Epoch 254, Loss: 14.677932710401532, Val-Loss: 14.859209531176639\n",
      "Epoch 255, Loss: 14.376427017100973, Val-Loss: 15.390506675406487\n",
      "Epoch 256, Loss: 16.795856004305236, Val-Loss: 14.690606663355078\n",
      "Epoch 257, Loss: 14.279195105578658, Val-Loss: 14.92039066120113\n",
      "Epoch 258, Loss: 14.895756642387617, Val-Loss: 14.906733120550191\n",
      "Epoch 259, Loss: 14.529176426346154, Val-Loss: 16.164276100215833\n",
      "Epoch 260, Loss: 14.810940505908086, Val-Loss: 17.482932594998633\n",
      "Epoch 261, Loss: 14.446497659664645, Val-Loss: 15.152830272605355\n",
      "Epoch 262, Loss: 14.62257290497512, Val-Loss: 15.664666433091982\n",
      "Epoch 263, Loss: 14.454363862169414, Val-Loss: 14.927770936783773\n",
      "Epoch 264, Loss: 14.540849545940224, Val-Loss: 17.183330830295557\n",
      "Epoch 265, Loss: 14.6943882245044, Val-Loss: 14.731897857081755\n",
      "Epoch 266, Loss: 14.267435111530117, Val-Loss: 16.183669999615507\n",
      "Epoch 267, Loss: 14.469739318212154, Val-Loss: 14.78829124937906\n",
      "Epoch 268, Loss: 14.31473112043895, Val-Loss: 15.339766410448512\n",
      "(new best)\n",
      "Epoch 269, Loss: 14.352817844609755, Val-Loss: 14.50591477988375\n",
      "Epoch 270, Loss: 14.280165519890625, Val-Loss: 14.777895358891348\n",
      "Epoch 271, Loss: 14.292715876073846, Val-Loss: 15.259980245981218\n",
      "Epoch 272, Loss: 15.261705126418875, Val-Loss: 14.929429340601404\n",
      "Epoch 273, Loss: 14.434503805583136, Val-Loss: 14.681900412425746\n",
      "Epoch 274, Loss: 14.911919684656796, Val-Loss: 14.747193858683879\n",
      "Epoch 275, Loss: 14.292229263584812, Val-Loss: 14.935175849184917\n",
      "Epoch 276, Loss: 14.396868734292463, Val-Loss: 14.943898158808835\n",
      "Epoch 277, Loss: 14.342367401151302, Val-Loss: 16.001597803576725\n",
      "Epoch 278, Loss: 14.300665570759937, Val-Loss: 22.151304723083953\n",
      "Epoch 279, Loss: 14.825632526640868, Val-Loss: 14.923303756261063\n",
      "Epoch 280, Loss: 14.205180917694525, Val-Loss: 14.724797211616426\n",
      "Epoch 281, Loss: 14.340712345053413, Val-Loss: 15.09882003780424\n",
      "Epoch 282, Loss: 14.100988999792364, Val-Loss: 15.357423371178154\n",
      "Epoch 283, Loss: 14.317086908082453, Val-Loss: 14.847977428725844\n",
      "Epoch 284, Loss: 14.32171685589737, Val-Loss: 14.93152319863504\n",
      "Epoch 285, Loss: 14.233831948049147, Val-Loss: 15.19167685203797\n",
      "Epoch 286, Loss: 15.231321063798033, Val-Loss: 14.616704956226299\n",
      "Epoch 287, Loss: 14.138205471521886, Val-Loss: 15.021015228381703\n",
      "Epoch 288, Loss: 14.165069338932154, Val-Loss: 14.74259944426893\n",
      "Epoch 289, Loss: 14.170160306644005, Val-Loss: 15.684731776683519\n",
      "Epoch 290, Loss: 14.169236682395692, Val-Loss: 22.832806651276314\n",
      "Epoch 291, Loss: 14.354097110344187, Val-Loss: 14.730317762210642\n",
      "Epoch 292, Loss: 14.29139861062822, Val-Loss: 15.719816063640131\n",
      "Epoch 293, Loss: 14.277780954085669, Val-Loss: 15.437997192586348\n",
      "Epoch 294, Loss: 15.203661431765315, Val-Loss: 14.983172721569852\n",
      "Epoch 295, Loss: 14.462320461997848, Val-Loss: 15.074971574957104\n",
      "Epoch 296, Loss: 14.10673099523866, Val-Loss: 19.09738978179602\n",
      "Epoch 297, Loss: 14.378662346762114, Val-Loss: 15.361483743394656\n",
      "Epoch 298, Loss: 14.14909095513429, Val-Loss: 15.422762874198991\n",
      "Epoch 299, Loss: 14.29930725919705, Val-Loss: 15.239060930922232\n",
      "Epoch 300, Loss: 14.088741911195406, Val-Loss: 15.950656344187756\n",
      "Epoch 301, Loss: 14.6092788590211, Val-Loss: 15.738308672145543\n",
      "Epoch 302, Loss: 14.008825764595318, Val-Loss: 15.08749708783631\n",
      "Epoch 303, Loss: 14.247657689196254, Val-Loss: 14.852776935559254\n",
      "Epoch 304, Loss: 14.122655952438, Val-Loss: 15.586385273699323\n",
      "Epoch 305, Loss: 14.099838840681347, Val-Loss: 14.826372043059454\n",
      "Epoch 306, Loss: 14.256434362490435, Val-Loss: 15.090569427771523\n",
      "Epoch 307, Loss: 14.095587338041996, Val-Loss: 14.774339184160812\n",
      "Epoch 308, Loss: 14.068543394894693, Val-Loss: 15.407629537247345\n",
      "Epoch 309, Loss: 14.148225346102597, Val-Loss: 14.62342487383296\n",
      "(new best)\n",
      "Epoch 310, Loss: 14.156340404888857, Val-Loss: 14.380387964975842\n",
      "Epoch 311, Loss: 14.072001651576807, Val-Loss: 15.77690045407419\n",
      "Epoch 312, Loss: 14.011865472472389, Val-Loss: 15.029429205023124\n",
      "Epoch 313, Loss: 14.023776008954414, Val-Loss: 14.6541947887487\n",
      "Epoch 314, Loss: 14.255866430891754, Val-Loss: 14.681851750390807\n",
      "Epoch 315, Loss: 13.923975991588648, Val-Loss: 14.726838882573594\n",
      "Epoch 316, Loss: 14.100258714092572, Val-Loss: 14.996853255611834\n",
      "Epoch 317, Loss: 14.104229654474576, Val-Loss: 15.39133872532267\n",
      "Epoch 318, Loss: 14.013594905319797, Val-Loss: 14.6870922757655\n",
      "Epoch 319, Loss: 14.02500313684648, Val-Loss: 14.723853010022488\n",
      "Epoch 320, Loss: 14.135061619210083, Val-Loss: 15.83002962866029\n",
      "Epoch 321, Loss: 14.355065131236293, Val-Loss: 15.215919455629448\n",
      "Epoch 322, Loss: 13.985026638105944, Val-Loss: 14.964458704915092\n",
      "Epoch 323, Loss: 14.15096173603086, Val-Loss: 14.983900465288192\n",
      "Epoch 324, Loss: 14.633700865958716, Val-Loss: 15.275077729099586\n",
      "Epoch 325, Loss: 13.939279534359388, Val-Loss: 15.440296050898814\n",
      "Epoch 326, Loss: 14.005303833121294, Val-Loss: 14.804721015862082\n",
      "Epoch 327, Loss: 14.157877283008759, Val-Loss: 18.339092406264687\n",
      "Epoch 328, Loss: 14.245126479667498, Val-Loss: 14.919729401362277\n",
      "Epoch 329, Loss: 13.904975199382726, Val-Loss: 15.065470057948922\n",
      "Epoch 330, Loss: 14.000499103809569, Val-Loss: 15.092358374744926\n",
      "Epoch 331, Loss: 14.253524002137368, Val-Loss: 15.463261381613284\n",
      "Epoch 332, Loss: 13.921016446675656, Val-Loss: 14.876341504376363\n",
      "Epoch 333, Loss: 14.42739252648914, Val-Loss: 15.078251028440556\n",
      "Epoch 334, Loss: 13.986962378678584, Val-Loss: 14.85811039787542\n",
      "Epoch 335, Loss: 14.090056305202536, Val-Loss: 14.695806962331824\n",
      "Epoch 336, Loss: 13.95078392159352, Val-Loss: 15.464696524282088\n",
      "Epoch 337, Loss: 14.160701082353299, Val-Loss: 14.60111589721965\n",
      "Epoch 338, Loss: 14.026378583506823, Val-Loss: 14.531023856958226\n",
      "Epoch 339, Loss: 14.053263162938968, Val-Loss: 15.223462143876237\n",
      "Epoch 340, Loss: 14.06415523945076, Val-Loss: 15.002434623366032\n",
      "Epoch 341, Loss: 14.042569258577913, Val-Loss: 14.460858504631156\n",
      "Epoch 342, Loss: 14.298241509422708, Val-Loss: 15.208316734392413\n",
      "Epoch 343, Loss: 14.038727537776673, Val-Loss: 14.6734628852276\n",
      "Epoch 344, Loss: 14.726721912881233, Val-Loss: 14.898396715059668\n",
      "Epoch 345, Loss: 13.812446103677736, Val-Loss: 14.564484267291698\n",
      "Epoch 346, Loss: 13.933190182073423, Val-Loss: 15.02311780866833\n",
      "Epoch 347, Loss: 14.168395306238175, Val-Loss: 14.818185435870099\n",
      "Epoch 348, Loss: 13.79804116537276, Val-Loss: 14.77780677678715\n",
      "Epoch 349, Loss: 14.099001653710292, Val-Loss: 15.042538631570315\n",
      "Epoch 350, Loss: 13.975121522628262, Val-Loss: 15.050018013392888\n",
      "Epoch 351, Loss: 14.150781290490334, Val-Loss: 14.706199040856907\n",
      "Epoch 352, Loss: 13.780863083205714, Val-Loss: 14.705212861242169\n",
      "Epoch 353, Loss: 13.942979843209285, Val-Loss: 14.579830744476137\n",
      "Epoch 354, Loss: 14.408153189437318, Val-Loss: 15.028064502215878\n",
      "Epoch 355, Loss: 13.76454644177633, Val-Loss: 14.475516567956744\n",
      "Epoch 356, Loss: 13.888390324723916, Val-Loss: 16.915205715894125\n",
      "Epoch 357, Loss: 14.115705807718754, Val-Loss: 15.617611895651024\n",
      "Epoch 358, Loss: 14.10043127491401, Val-Loss: 15.17671982666138\n",
      "Epoch 359, Loss: 14.068972236305823, Val-Loss: 14.902562279824677\n",
      "Epoch 360, Loss: 13.844022020914636, Val-Loss: 14.968628301400267\n",
      "Epoch 361, Loss: 13.834469415118425, Val-Loss: 14.708461198542253\n",
      "Epoch 362, Loss: 13.813326982137719, Val-Loss: 15.519905334708605\n",
      "Epoch 363, Loss: 13.891195948060135, Val-Loss: 15.156595191044017\n",
      "Epoch 364, Loss: 13.873139465488983, Val-Loss: 14.548450489444924\n",
      "Epoch 365, Loss: 13.824408832663623, Val-Loss: 14.789108733133657\n",
      "Epoch 366, Loss: 13.943172504984755, Val-Loss: 15.277822918601114\n",
      "Epoch 367, Loss: 13.888779052576607, Val-Loss: 14.909827438558901\n",
      "Epoch 368, Loss: 13.759458626985086, Val-Loss: 15.890543417202666\n",
      "Epoch 369, Loss: 15.796594899571012, Val-Loss: 16.725210268121003\n",
      "Epoch 370, Loss: 14.155443552541053, Val-Loss: 14.626347670751402\n",
      "Epoch 371, Loss: 13.785305391261248, Val-Loss: 14.81522183611657\n",
      "Epoch 372, Loss: 13.90926291097072, Val-Loss: 14.686860297682548\n",
      "Epoch 373, Loss: 13.808527794997362, Val-Loss: 14.490040293291985\n",
      "Epoch 374, Loss: 13.891073353949555, Val-Loss: 16.078414214145262\n",
      "Epoch 375, Loss: 14.028943362455392, Val-Loss: 14.990533681458135\n",
      "Epoch 376, Loss: 14.418065340364823, Val-Loss: 14.679439377065219\n",
      "Epoch 377, Loss: 13.813887504298654, Val-Loss: 14.504695572250675\n",
      "Epoch 378, Loss: 13.832779290808471, Val-Loss: 14.605036992274279\n",
      "Epoch 379, Loss: 13.666357171165417, Val-Loss: 15.162965019817024\n",
      "Epoch 380, Loss: 13.709053609530114, Val-Loss: 15.030425836794823\n",
      "Epoch 381, Loss: 14.489182523079823, Val-Loss: 15.493213551986809\n",
      "Epoch 382, Loss: 13.861337751158837, Val-Loss: 14.765382352237035\n",
      "Epoch 383, Loss: 13.826394842811835, Val-Loss: 14.890707765316392\n",
      "Epoch 384, Loss: 14.007394753918039, Val-Loss: 14.78098907078194\n",
      "Epoch 385, Loss: 13.783055635496787, Val-Loss: 14.60352494483135\n",
      "Epoch 386, Loss: 13.744746194165383, Val-Loss: 14.425603625781253\n",
      "Epoch 387, Loss: 13.827476002472931, Val-Loss: 14.765767245566543\n",
      "Epoch 388, Loss: 13.753831951712419, Val-Loss: 14.463537341580887\n",
      "Epoch 389, Loss: 13.711982186021757, Val-Loss: 15.27622687949837\n",
      "Epoch 390, Loss: 13.761454351631675, Val-Loss: 15.06320656635163\n",
      "Epoch 391, Loss: 13.705833731180201, Val-Loss: 14.679456514512946\n",
      "Epoch 392, Loss: 14.070607142699178, Val-Loss: 14.86732228729079\n",
      "Epoch 393, Loss: 13.76122687457986, Val-Loss: 15.441343045083007\n",
      "Epoch 394, Loss: 13.791697975644773, Val-Loss: 14.715542819070446\n",
      "Epoch 395, Loss: 13.708438824337264, Val-Loss: 14.963678113616325\n",
      "Epoch 396, Loss: 13.813694826090291, Val-Loss: 14.597900884612073\n",
      "Epoch 397, Loss: 13.73077502649742, Val-Loss: 14.690190264063665\n",
      "Epoch 398, Loss: 13.80386822435681, Val-Loss: 14.742770786498097\n",
      "Epoch 399, Loss: 13.787368236889149, Val-Loss: 15.385375570576093\n",
      "Epoch 400, Loss: 13.973613745898213, Val-Loss: 14.689960823012518\n",
      "Epoch 401, Loss: 13.679697208327447, Val-Loss: 14.477477904207035\n",
      "Epoch 402, Loss: 13.732137598927906, Val-Loss: 14.587159820676712\n",
      "Epoch 403, Loss: 13.666757480703776, Val-Loss: 14.687487491290085\n",
      "Epoch 404, Loss: 13.840514926233856, Val-Loss: 15.288036531785439\n",
      "Epoch 405, Loss: 13.737301083933575, Val-Loss: 14.836816022799242\n",
      "Epoch 406, Loss: 13.707071010314374, Val-Loss: 14.722408433798254\n",
      "Epoch 407, Loss: 13.788625611343761, Val-Loss: 14.916050579580366\n",
      "Epoch 408, Loss: 14.67532033696689, Val-Loss: 14.986528097806385\n",
      "Epoch 409, Loss: 13.620173430381747, Val-Loss: 14.795411146630421\n",
      "Epoch 410, Loss: 13.6957101832422, Val-Loss: 15.199840484075693\n",
      "Epoch 411, Loss: 13.684827772967013, Val-Loss: 15.806056090851232\n",
      "Epoch 412, Loss: 13.703585581826989, Val-Loss: 14.800048244915155\n",
      "Epoch 413, Loss: 13.645770255362327, Val-Loss: 15.551539965326313\n",
      "Epoch 414, Loss: 13.73414945387191, Val-Loss: 14.949962877511174\n",
      "Epoch 415, Loss: 13.673293541792187, Val-Loss: 14.513273078277743\n",
      "Epoch 416, Loss: 13.658310213595088, Val-Loss: 14.635240708578209\n",
      "Epoch 417, Loss: 13.773610044879725, Val-Loss: 14.820833327696956\n",
      "Epoch 418, Loss: 13.712263062197893, Val-Loss: 14.757484257241272\n",
      "Epoch 419, Loss: 13.599116017844095, Val-Loss: 14.936304078456665\n",
      "Epoch 420, Loss: 13.7839855966147, Val-Loss: 14.459847181227385\n",
      "Epoch 421, Loss: 13.669780919786739, Val-Loss: 14.546018787665478\n",
      "Epoch 422, Loss: 13.726756796895582, Val-Loss: 14.641745920276824\n",
      "Epoch 423, Loss: 13.767347044708302, Val-Loss: 14.731839640929376\n",
      "Epoch 424, Loss: 13.628219174377458, Val-Loss: 14.833710809209823\n",
      "Epoch 425, Loss: 13.663889380085665, Val-Loss: 14.732033022491153\n",
      "Epoch 426, Loss: 13.741099451314355, Val-Loss: 14.940329667656103\n",
      "Epoch 427, Loss: 14.176996368254386, Val-Loss: 15.036543637130636\n",
      "Epoch 428, Loss: 13.597580713887977, Val-Loss: 15.582390832123515\n",
      "Epoch 429, Loss: 13.579978138919863, Val-Loss: 15.08836906929265\n",
      "Epoch 430, Loss: 13.599950561113245, Val-Loss: 14.586078827688917\n",
      "Epoch 431, Loss: 13.48353990503855, Val-Loss: 14.53503822584334\n",
      "Epoch 432, Loss: 13.566970719348552, Val-Loss: 14.758178364860603\n",
      "(new best)\n",
      "Epoch 433, Loss: 13.90038966430012, Val-Loss: 14.376357205287084\n",
      "Epoch 434, Loss: 13.745382289047608, Val-Loss: 14.769699047396392\n",
      "Epoch 435, Loss: 14.524974130523674, Val-Loss: 16.296660439068717\n",
      "Epoch 436, Loss: 13.670671874056572, Val-Loss: 14.71992006011798\n",
      "(new best)\n",
      "Epoch 437, Loss: 13.651152684725634, Val-Loss: 14.302512885191733\n",
      "Epoch 438, Loss: 13.55720943731817, Val-Loss: 15.456670313916122\n",
      "Epoch 439, Loss: 13.596265807689845, Val-Loss: 14.464022720528975\n",
      "Epoch 440, Loss: 13.68218340383551, Val-Loss: 14.95306328426759\n",
      "Epoch 441, Loss: 13.612879008873554, Val-Loss: 14.84038439333572\n",
      "Epoch 442, Loss: 13.557880215927385, Val-Loss: 14.478314433399351\n",
      "Epoch 443, Loss: 13.566714746816752, Val-Loss: 14.530974815340063\n",
      "Epoch 444, Loss: 13.66973503928408, Val-Loss: 14.694408758742474\n",
      "Epoch 445, Loss: 13.586741792442316, Val-Loss: 14.747818869342476\n",
      "Epoch 446, Loss: 13.670375833075141, Val-Loss: 14.501791028500003\n",
      "Epoch 447, Loss: 13.587669105274928, Val-Loss: 15.692847235033508\n",
      "Epoch 448, Loss: 13.625615992296083, Val-Loss: 15.199631605380404\n",
      "Epoch 449, Loss: 13.633313267602231, Val-Loss: 14.596107038941508\n",
      "Epoch 450, Loss: 13.673631422251695, Val-Loss: 14.810522441867704\n",
      "Epoch 451, Loss: 13.555524556123114, Val-Loss: 14.57027513933858\n",
      "Epoch 452, Loss: 13.601471157198645, Val-Loss: 14.674580926004804\n",
      "Epoch 453, Loss: 13.619396163653951, Val-Loss: 14.836696419170481\n",
      "Epoch 454, Loss: 13.533661518536482, Val-Loss: 14.446740543931659\n",
      "Epoch 455, Loss: 14.34691467772791, Val-Loss: 14.87684026994853\n",
      "Epoch 456, Loss: 13.53186703227621, Val-Loss: 14.44781663315313\n",
      "Epoch 457, Loss: 13.597265783937344, Val-Loss: 14.789158294175415\n",
      "Epoch 458, Loss: 13.522372680362327, Val-Loss: 14.89773509226722\n",
      "Epoch 459, Loss: 15.442952452992067, Val-Loss: 14.868220296683068\n",
      "Epoch 460, Loss: 13.73877013588246, Val-Loss: 14.579520983755774\n",
      "Epoch 461, Loss: 13.470816384998408, Val-Loss: 14.659817343126901\n",
      "Epoch 462, Loss: 13.58573171974836, Val-Loss: 15.014122168379743\n",
      "Epoch 463, Loss: 13.499918691669151, Val-Loss: 14.797041711401764\n",
      "Epoch 464, Loss: 13.49481540771446, Val-Loss: 14.447251127820431\n",
      "Epoch 465, Loss: 14.214179659447327, Val-Loss: 14.478019486162328\n",
      "Epoch 466, Loss: 13.461440100477645, Val-Loss: 14.391299844981619\n",
      "Epoch 467, Loss: 13.495532276000377, Val-Loss: 14.777057978574714\n",
      "Epoch 468, Loss: 13.496611894048918, Val-Loss: 14.680056286788489\n",
      "Epoch 469, Loss: 13.489785926180486, Val-Loss: 14.897493728314426\n",
      "Epoch 470, Loss: 13.49807575448156, Val-Loss: 15.307030151785613\n",
      "(new best)\n",
      "Epoch 471, Loss: 13.595096916971015, Val-Loss: 14.208324679433161\n",
      "Epoch 472, Loss: 13.410602430814773, Val-Loss: 14.506456847392773\n",
      "Epoch 473, Loss: 13.83422845951623, Val-Loss: 14.287122934281514\n",
      "Epoch 474, Loss: 13.518643626292628, Val-Loss: 15.244564683521958\n",
      "Epoch 475, Loss: 13.460498556703886, Val-Loss: 14.929920762003688\n",
      "Epoch 476, Loss: 13.479591909013768, Val-Loss: 14.910121342819162\n",
      "Epoch 477, Loss: 13.977301571928148, Val-Loss: 14.66565051925336\n",
      "Epoch 478, Loss: 13.426163817344888, Val-Loss: 14.75171059070822\n",
      "Epoch 479, Loss: 13.447373079234389, Val-Loss: 14.57714204566636\n",
      "Epoch 480, Loss: 13.473101960677491, Val-Loss: 14.546939772307065\n",
      "Epoch 481, Loss: 13.428902392923078, Val-Loss: 14.74398182773196\n",
      "Epoch 482, Loss: 13.452210126254412, Val-Loss: 14.93239900544862\n",
      "Epoch 483, Loss: 13.528764828032399, Val-Loss: 14.324575076645823\n",
      "Epoch 484, Loss: 13.397284547805267, Val-Loss: 14.807311815074035\n",
      "Epoch 485, Loss: 13.419972136954653, Val-Loss: 14.824381282177788\n",
      "Epoch 486, Loss: 13.390380091004678, Val-Loss: 15.264272501705916\n",
      "Epoch 487, Loss: 13.420253669128723, Val-Loss: 14.885154747637662\n",
      "Epoch 488, Loss: 13.430565244478604, Val-Loss: 14.90101030675739\n",
      "Epoch 489, Loss: 13.474867271029403, Val-Loss: 14.790622847103188\n",
      "Epoch 490, Loss: 13.457481018613061, Val-Loss: 15.47113537609416\n",
      "Epoch 491, Loss: 13.464282011084387, Val-Loss: 14.871154861078155\n",
      "Epoch 492, Loss: 13.445384081167662, Val-Loss: 14.360111535861453\n",
      "Epoch 493, Loss: 13.434915266926426, Val-Loss: 14.57176101737941\n",
      "Epoch 494, Loss: 13.448855782578283, Val-Loss: 15.240878455671476\n",
      "Epoch 495, Loss: 13.487884647325235, Val-Loss: 14.783782718399467\n",
      "Epoch 496, Loss: 13.447838514724964, Val-Loss: 14.504904913162\n",
      "Epoch 497, Loss: 13.514587312527475, Val-Loss: 14.676111953977923\n",
      "Epoch 498, Loss: 13.763813221640739, Val-Loss: 15.252471093364614\n",
      "Epoch 499, Loss: 13.419244727733963, Val-Loss: 14.725555332810144\n",
      "Epoch 500, Loss: 13.423309144253832, Val-Loss: 14.510836063436203\n",
      "Epoch 501, Loss: 13.55695295094317, Val-Loss: 14.280817004408199\n",
      "Epoch 502, Loss: 13.43944381493153, Val-Loss: 14.41067884492362\n",
      "Epoch 503, Loss: 13.383474295689533, Val-Loss: 14.843822542395339\n",
      "Epoch 504, Loss: 13.367587201319239, Val-Loss: 14.33048448586959\n",
      "Epoch 505, Loss: 13.3829777903676, Val-Loss: 14.307249230858519\n",
      "Epoch 506, Loss: 13.396116459436698, Val-Loss: 14.92526054176767\n",
      "Epoch 507, Loss: 13.628884358057501, Val-Loss: 14.53333386603444\n",
      "Epoch 508, Loss: 13.361415378858364, Val-Loss: 14.597123542884226\n",
      "Epoch 509, Loss: 13.658418606807535, Val-Loss: 14.353039810902679\n",
      "Epoch 510, Loss: 13.354403728875853, Val-Loss: 14.4768628491545\n",
      "Epoch 511, Loss: 13.316429805660514, Val-Loss: 14.604024427500608\n",
      "Epoch 512, Loss: 13.38844746566128, Val-Loss: 14.543331108673623\n",
      "Epoch 513, Loss: 13.566459903984969, Val-Loss: 14.525206471231897\n",
      "Epoch 514, Loss: 13.403665271378353, Val-Loss: 14.514547849224574\n",
      "Epoch 515, Loss: 13.37687595726322, Val-Loss: 14.237945637884103\n",
      "Epoch 516, Loss: 13.303294683225314, Val-Loss: 14.612516327941757\n",
      "Epoch 517, Loss: 13.354148312442254, Val-Loss: 14.733571589477629\n",
      "Epoch 518, Loss: 13.364604577413898, Val-Loss: 15.311337835992592\n",
      "Epoch 519, Loss: 13.407763641379761, Val-Loss: 14.399831445350499\n",
      "Epoch 520, Loss: 13.327787816403811, Val-Loss: 15.122370862426534\n",
      "Epoch 521, Loss: 13.366569382025737, Val-Loss: 14.340998979533119\n",
      "Epoch 522, Loss: 14.01552004767423, Val-Loss: 14.463164743980776\n",
      "Epoch 523, Loss: 14.243700395712027, Val-Loss: 14.574614740638756\n",
      "Epoch 524, Loss: 13.341015433863461, Val-Loss: 14.262233227832446\n",
      "Epoch 525, Loss: 13.425019681379986, Val-Loss: 14.685349724377344\n",
      "Epoch 526, Loss: 13.450726159238819, Val-Loss: 14.520715157507937\n",
      "Epoch 527, Loss: 13.313168448405401, Val-Loss: 14.739213045059946\n",
      "Epoch 528, Loss: 13.335701337933518, Val-Loss: 15.054575032174105\n",
      "Epoch 529, Loss: 13.373586573111325, Val-Loss: 14.563594940043531\n",
      "Epoch 530, Loss: 13.340907654095249, Val-Loss: 14.52450440939655\n",
      "Epoch 531, Loss: 13.31088446113067, Val-Loss: 14.607001733073162\n",
      "Epoch 532, Loss: 13.298101236713224, Val-Loss: 15.162472451609071\n",
      "Epoch 533, Loss: 13.336180906921504, Val-Loss: 14.85195050490669\n",
      "Epoch 534, Loss: 13.362222552355732, Val-Loss: 14.347579962707067\n",
      "Epoch 535, Loss: 13.37010941188838, Val-Loss: 14.89314331539098\n",
      "Epoch 536, Loss: 13.328816870781743, Val-Loss: 15.095175926361586\n",
      "Epoch 537, Loss: 13.306413081402269, Val-Loss: 14.59629836809622\n",
      "Epoch 538, Loss: 13.449830225591468, Val-Loss: 14.310249889168412\n",
      "Epoch 539, Loss: 13.378786993587845, Val-Loss: 14.504149860842684\n",
      "Epoch 540, Loss: 13.296127170150555, Val-Loss: 14.437619348053795\n",
      "Epoch 541, Loss: 13.280386836191283, Val-Loss: 14.30010968469526\n",
      "Epoch 542, Loss: 13.235516008660491, Val-Loss: 14.649756008580114\n",
      "Epoch 543, Loss: 13.30823576481418, Val-Loss: 14.457587102075074\n",
      "Epoch 544, Loss: 13.261672176758157, Val-Loss: 14.67176705435267\n",
      "Epoch 545, Loss: 13.276720161232245, Val-Loss: 14.436588110074284\n",
      "Epoch 546, Loss: 13.435800492260608, Val-Loss: 14.495162280311643\n",
      "Epoch 547, Loss: 13.251357902074274, Val-Loss: 14.412621062986831\n",
      "Epoch 548, Loss: 13.233547473126777, Val-Loss: 14.525539404606425\n",
      "Epoch 549, Loss: 13.269166070385417, Val-Loss: 14.306624584116495\n",
      "Epoch 550, Loss: 13.285462509363928, Val-Loss: 14.541519405385717\n",
      "Epoch 551, Loss: 13.256864483626604, Val-Loss: 14.295811251400611\n",
      "Epoch 552, Loss: 13.30749117825152, Val-Loss: 14.235432626613063\n",
      "Epoch 553, Loss: 13.196409876880267, Val-Loss: 15.057882057028573\n",
      "Epoch 554, Loss: 13.262368065047905, Val-Loss: 14.62539369863438\n",
      "Epoch 555, Loss: 13.224037936617707, Val-Loss: 14.549005438873825\n",
      "Epoch 556, Loss: 13.266291837073236, Val-Loss: 14.32558501627469\n",
      "Epoch 557, Loss: 13.269417283709076, Val-Loss: 14.684938930168718\n",
      "Epoch 558, Loss: 13.23539935141237, Val-Loss: 14.927327348543063\n",
      "Epoch 559, Loss: 13.241630357053639, Val-Loss: 14.634856162412158\n",
      "Epoch 560, Loss: 13.276731992865702, Val-Loss: 14.543448810100655\n",
      "Epoch 561, Loss: 13.270462240062551, Val-Loss: 14.464849730962307\n",
      "Epoch 562, Loss: 13.255302613119321, Val-Loss: 14.451500572400827\n",
      "Epoch 563, Loss: 13.198942701086857, Val-Loss: 14.530655793138893\n",
      "Epoch 564, Loss: 13.26626245673383, Val-Loss: 14.403373524752128\n",
      "Epoch 565, Loss: 13.272622916018022, Val-Loss: 14.45201584537891\n",
      "Epoch 566, Loss: 13.203503696977627, Val-Loss: 15.712021847953242\n",
      "Epoch 567, Loss: 13.210758666110461, Val-Loss: 14.872210117634808\n",
      "Epoch 568, Loss: 13.204239582125965, Val-Loss: 14.847896340844766\n",
      "Epoch 569, Loss: 13.237355419428603, Val-Loss: 14.508315322526476\n",
      "Epoch 570, Loss: 13.211391125792469, Val-Loss: 15.022471732621886\n",
      "Epoch 571, Loss: 13.210219141710102, Val-Loss: 14.423320880316336\n",
      "Epoch 572, Loss: 13.168136550506121, Val-Loss: 14.524047725509138\n",
      "Epoch 573, Loss: 13.230072903119742, Val-Loss: 15.215613488160837\n",
      "Epoch 574, Loss: 13.244456363703254, Val-Loss: 15.797841484683524\n",
      "Epoch 575, Loss: 13.294166412206021, Val-Loss: 14.747953307523602\n",
      "Epoch 576, Loss: 13.494197273552778, Val-Loss: 14.229250483091787\n",
      "Epoch 577, Loss: 13.212291038887805, Val-Loss: 14.496545441846933\n",
      "Epoch 578, Loss: 13.165464008328247, Val-Loss: 14.668519189801977\n",
      "Epoch 579, Loss: 13.181775389574524, Val-Loss: 15.567665489451922\n",
      "(new best)\n",
      "Epoch 580, Loss: 13.203775049364562, Val-Loss: 14.163547375683775\n",
      "Epoch 581, Loss: 13.201219755596103, Val-Loss: 14.552442994030807\n",
      "Epoch 582, Loss: 13.185661291963285, Val-Loss: 14.292009222172014\n",
      "(new best)\n",
      "Epoch 583, Loss: 13.158789006575509, Val-Loss: 14.074889457207162\n",
      "Epoch 584, Loss: 13.815202623768524, Val-Loss: 14.31290692628384\n",
      "Epoch 585, Loss: 13.157021619577218, Val-Loss: 14.567090137708096\n",
      "Epoch 586, Loss: 13.159186938908123, Val-Loss: 14.44437801036265\n",
      "Epoch 587, Loss: 13.146273108401187, Val-Loss: 14.304032444301392\n",
      "Epoch 588, Loss: 13.181214239007392, Val-Loss: 14.804831899551875\n",
      "Epoch 589, Loss: 13.164418609600883, Val-Loss: 14.478550337668644\n",
      "Epoch 590, Loss: 13.196344889513918, Val-Loss: 14.841952597288968\n",
      "Epoch 591, Loss: 13.18091842234003, Val-Loss: 14.45831626793145\n",
      "Epoch 592, Loss: 13.179834474700401, Val-Loss: 14.666903342945284\n",
      "Epoch 593, Loss: 13.146114615416092, Val-Loss: 14.346078493979448\n",
      "Epoch 594, Loss: 13.154597639659753, Val-Loss: 14.318651572684436\n",
      "Epoch 595, Loss: 13.132049449033918, Val-Loss: 14.307685303160378\n",
      "Epoch 596, Loss: 13.150072158995675, Val-Loss: 14.339545891935636\n",
      "Epoch 597, Loss: 13.136898172673229, Val-Loss: 14.483046463736368\n",
      "Epoch 598, Loss: 13.139276904754132, Val-Loss: 14.8420583961922\n",
      "Epoch 599, Loss: 13.133430331241343, Val-Loss: 14.129445361531284\n",
      "Epoch 600, Loss: 13.135369749698988, Val-Loss: 14.719179089663486\n",
      "Epoch 601, Loss: 13.13430889158116, Val-Loss: 14.280799603687184\n",
      "Epoch 602, Loss: 13.133775586068609, Val-Loss: 14.32837612429019\n",
      "Epoch 603, Loss: 13.162374863889022, Val-Loss: 14.84077493437852\n",
      "Epoch 604, Loss: 13.141629188720293, Val-Loss: 14.38490531982414\n",
      "Epoch 605, Loss: 13.164559265801767, Val-Loss: 14.372639265202125\n",
      "Epoch 606, Loss: 13.136049557789867, Val-Loss: 14.538635715868102\n",
      "Epoch 607, Loss: 13.137624429664804, Val-Loss: 14.42415596481992\n",
      "Epoch 608, Loss: 13.139569916114162, Val-Loss: 14.783495864829904\n",
      "Epoch 609, Loss: 13.126306038352942, Val-Loss: 14.601483520477924\n",
      "Epoch 610, Loss: 13.132836640171375, Val-Loss: 14.462200048082655\n",
      "Epoch 611, Loss: 13.143135757936891, Val-Loss: 14.487173199958114\n",
      "Epoch 612, Loss: 13.127907817206525, Val-Loss: 14.342804562013661\n",
      "Epoch 613, Loss: 13.107036906670551, Val-Loss: 15.042363661149295\n",
      "Epoch 614, Loss: 13.087856743056, Val-Loss: 14.336802417594358\n",
      "Epoch 615, Loss: 13.10023300812571, Val-Loss: 14.321884467908376\n",
      "Epoch 616, Loss: 13.137147498935109, Val-Loss: 14.520710773784057\n",
      "Epoch 617, Loss: 13.084366041520958, Val-Loss: 14.317870230782082\n",
      "Epoch 618, Loss: 13.132200033568632, Val-Loss: 14.301055586947633\n",
      "Epoch 619, Loss: 13.098743750939171, Val-Loss: 14.343813923395382\n",
      "Epoch 620, Loss: 13.122821989470737, Val-Loss: 14.601216025814438\n",
      "Epoch 621, Loss: 13.602745141630784, Val-Loss: 14.334285611104269\n",
      "Epoch 622, Loss: 13.088848552324752, Val-Loss: 14.68032853491325\n",
      "Epoch 623, Loss: 13.28992561054709, Val-Loss: 14.486995749352106\n",
      "Epoch 624, Loss: 13.103212712331372, Val-Loss: 14.263803769374531\n",
      "Epoch 625, Loss: 13.082086590025607, Val-Loss: 14.509224178528406\n",
      "Epoch 626, Loss: 13.10100055845919, Val-Loss: 14.534869486910624\n",
      "Epoch 627, Loss: 13.099456427710587, Val-Loss: 14.813438616371863\n",
      "Epoch 628, Loss: 13.22269236142789, Val-Loss: 14.311259243770627\n",
      "Epoch 629, Loss: 13.095276301997984, Val-Loss: 14.399057058105631\n",
      "Epoch 630, Loss: 13.08367187389442, Val-Loss: 14.429261076389379\n",
      "Epoch 631, Loss: 13.089816171290977, Val-Loss: 14.45131792250672\n",
      "Epoch 632, Loss: 13.062598285894577, Val-Loss: 14.636581280373035\n",
      "Epoch 633, Loss: 13.082263195321998, Val-Loss: 14.324532464495928\n",
      "Epoch 634, Loss: 13.050049997657858, Val-Loss: 14.209767826487992\n",
      "Epoch 635, Loss: 13.085211557078814, Val-Loss: 14.214065901798008\n",
      "Epoch 636, Loss: 13.049481162507746, Val-Loss: 14.691266397734983\n",
      "Epoch 637, Loss: 13.070672442946314, Val-Loss: 14.615021619168644\n",
      "Epoch 638, Loss: 13.090478637022764, Val-Loss: 14.28339709634449\n",
      "Epoch 639, Loss: 13.075088174758434, Val-Loss: 14.185455405194036\n",
      "Epoch 640, Loss: 13.060834413271266, Val-Loss: 14.30281261622841\n",
      "Epoch 641, Loss: 13.035799958183604, Val-Loss: 14.256442190978882\n",
      "Epoch 642, Loss: 13.1463171706132, Val-Loss: 14.484204047573325\n",
      "Epoch 643, Loss: 13.024529228267191, Val-Loss: 14.632292946941654\n",
      "Epoch 644, Loss: 13.19576183944678, Val-Loss: 14.803206616603488\n",
      "Epoch 645, Loss: 13.056869996790711, Val-Loss: 14.41452411026592\n",
      "Epoch 646, Loss: 13.043766847192902, Val-Loss: 14.622855923196296\n",
      "Epoch 647, Loss: 13.024799440564578, Val-Loss: 14.322561486396426\n",
      "Epoch 648, Loss: 13.058032360313698, Val-Loss: 14.621840787801043\n",
      "Epoch 649, Loss: 13.044746044814717, Val-Loss: 14.40997309957892\n",
      "Epoch 650, Loss: 13.255388495177716, Val-Loss: 14.423522103643256\n",
      "Epoch 651, Loss: 13.055354824855078, Val-Loss: 14.244213882743065\n",
      "Epoch 652, Loss: 13.051077395252628, Val-Loss: 14.5779023199925\n",
      "Epoch 653, Loss: 13.016171459221932, Val-Loss: 14.504325223369571\n",
      "Epoch 654, Loss: 13.016741288341363, Val-Loss: 14.29336452836996\n",
      "Epoch 655, Loss: 13.06378748192025, Val-Loss: 14.389168498543047\n",
      "Epoch 656, Loss: 13.026406104598603, Val-Loss: 14.220116753958573\n",
      "Epoch 657, Loss: 13.043129492006704, Val-Loss: 14.74415643544172\n",
      "Epoch 658, Loss: 13.026570336418297, Val-Loss: 14.463688260416035\n",
      "Epoch 659, Loss: 13.067941536367112, Val-Loss: 14.61132985776029\n",
      "Epoch 660, Loss: 13.025080936184976, Val-Loss: 14.48729160027123\n",
      "Epoch 661, Loss: 13.045291151553494, Val-Loss: 14.433976230082857\n",
      "Epoch 662, Loss: 13.025378837435907, Val-Loss: 14.416721907781838\n",
      "Epoch 663, Loss: 13.043654431831843, Val-Loss: 14.798441055786562\n",
      "Epoch 664, Loss: 13.021759868675577, Val-Loss: 14.229112820547646\n",
      "Epoch 665, Loss: 13.034707461365011, Val-Loss: 14.364322450338461\n",
      "Epoch 666, Loss: 13.023629137574764, Val-Loss: 14.36820038416225\n",
      "Epoch 667, Loss: 13.009587150246201, Val-Loss: 14.36530659499381\n",
      "Epoch 668, Loss: 13.01570952079024, Val-Loss: 14.414037705935744\n",
      "Epoch 669, Loss: 12.985239893306296, Val-Loss: 14.385169823759457\n",
      "Epoch 670, Loss: 13.03402125161557, Val-Loss: 14.50769146620356\n",
      "Epoch 671, Loss: 13.000667065261315, Val-Loss: 14.357479387286897\n",
      "Epoch 672, Loss: 13.01242432410835, Val-Loss: 14.314876596698852\n",
      "Epoch 673, Loss: 12.989851509915477, Val-Loss: 14.446409762170854\n",
      "Epoch 674, Loss: 12.989390539177665, Val-Loss: 14.58168108040595\n",
      "Epoch 675, Loss: 13.003657618641899, Val-Loss: 14.471550055172646\n",
      "Epoch 676, Loss: 13.001956401564305, Val-Loss: 14.333766644451973\n",
      "Epoch 677, Loss: 12.998964721092285, Val-Loss: 14.454341612846424\n",
      "Epoch 678, Loss: 13.001215023191058, Val-Loss: 14.271483743009922\n",
      "Epoch 679, Loss: 12.994245895759786, Val-Loss: 14.38386113874916\n",
      "Epoch 680, Loss: 12.991664741571551, Val-Loss: 14.179560988036606\n",
      "Epoch 681, Loss: 12.969127177561225, Val-Loss: 14.538269698200281\n",
      "Epoch 682, Loss: 13.00143235886757, Val-Loss: 14.390311199065334\n",
      "Epoch 683, Loss: 12.995816762839636, Val-Loss: 14.277695084952974\n",
      "Epoch 684, Loss: 12.971984196375404, Val-Loss: 14.325924536967461\n",
      "Epoch 685, Loss: 12.992484939338464, Val-Loss: 14.315308441093434\n",
      "Epoch 686, Loss: 12.969652439640472, Val-Loss: 14.484578686329762\n",
      "Epoch 687, Loss: 12.985493551413606, Val-Loss: 14.410000569190515\n",
      "Epoch 688, Loss: 12.964209892031361, Val-Loss: 14.369163744260893\n",
      "Epoch 689, Loss: 12.982901370608891, Val-Loss: 14.266889408642975\n",
      "Epoch 690, Loss: 12.9695382825813, Val-Loss: 14.432181509624192\n",
      "Epoch 691, Loss: 12.989196963859825, Val-Loss: 14.469438064595819\n",
      "Epoch 692, Loss: 13.09118700183981, Val-Loss: 14.445403832294069\n",
      "Epoch 693, Loss: 12.964538647776797, Val-Loss: 14.158715217437198\n",
      "Epoch 694, Loss: 12.97897160709553, Val-Loss: 14.43749929796406\n",
      "Epoch 695, Loss: 12.979393808785197, Val-Loss: 14.302852942537712\n",
      "Epoch 696, Loss: 12.952401943996085, Val-Loss: 14.531679387384274\n",
      "Epoch 697, Loss: 12.940032624454586, Val-Loss: 14.611878694846798\n",
      "Epoch 698, Loss: 13.07115929765446, Val-Loss: 14.8938162330098\n",
      "Epoch 699, Loss: 12.959820738356129, Val-Loss: 14.12787272385257\n",
      "Epoch 700, Loss: 12.956331357905794, Val-Loss: 14.193189152142773\n",
      "Epoch 701, Loss: 12.961093963026086, Val-Loss: 14.440130441939248\n",
      "Epoch 702, Loss: 12.979027928835295, Val-Loss: 14.341353962973757\n",
      "Epoch 703, Loss: 12.945132517681037, Val-Loss: 14.797728147855878\n",
      "Epoch 704, Loss: 12.968229374156447, Val-Loss: 14.377756646784643\n",
      "Epoch 705, Loss: 12.925022771039327, Val-Loss: 14.393582858525152\n",
      "Epoch 706, Loss: 12.940586854844975, Val-Loss: 14.459988625838204\n",
      "Epoch 707, Loss: 12.94183747181141, Val-Loss: 14.462676024475185\n",
      "Epoch 708, Loss: 12.940968591201042, Val-Loss: 14.512042838039271\n",
      "Epoch 709, Loss: 13.065675343549719, Val-Loss: 14.450159034839029\n",
      "Epoch 710, Loss: 12.935474303495836, Val-Loss: 14.75784732948029\n",
      "Epoch 711, Loss: 12.942664560494793, Val-Loss: 14.381475821601814\n",
      "Epoch 712, Loss: 12.927543272020385, Val-Loss: 14.292970912461778\n",
      "Epoch 713, Loss: 13.043939751631575, Val-Loss: 14.481317462789585\n",
      "Epoch 714, Loss: 12.930561331296243, Val-Loss: 14.457350924414909\n",
      "Epoch 715, Loss: 12.923278184959118, Val-Loss: 14.575041787153033\n",
      "Epoch 716, Loss: 12.918689131057826, Val-Loss: 14.245330707606948\n",
      "Epoch 717, Loss: 12.939165918248031, Val-Loss: 14.3334572296971\n",
      "Epoch 718, Loss: 12.927520779957526, Val-Loss: 14.226204929740662\n",
      "Epoch 719, Loss: 12.933057940697791, Val-Loss: 14.572151087293108\n",
      "Epoch 720, Loss: 12.944317211534413, Val-Loss: 14.33861249831535\n",
      "Epoch 721, Loss: 12.935696195085269, Val-Loss: 14.26729582153528\n",
      "Epoch 722, Loss: 12.935226337028713, Val-Loss: 14.419129386727953\n",
      "Epoch 723, Loss: 12.923691905889449, Val-Loss: 14.346228165154983\n",
      "Epoch 724, Loss: 12.937560263632912, Val-Loss: 14.38476650049305\n",
      "Epoch 725, Loss: 12.911539757714303, Val-Loss: 14.675242330683945\n",
      "Epoch 726, Loss: 12.923995760841764, Val-Loss: 15.250706915074932\n",
      "Epoch 727, Loss: 12.91325829864801, Val-Loss: 14.410244505652708\n",
      "Epoch 728, Loss: 12.931037806827312, Val-Loss: 14.284205636689164\n",
      "Epoch 729, Loss: 12.912510915204773, Val-Loss: 14.702209952323134\n",
      "Epoch 730, Loss: 12.901532657411646, Val-Loss: 14.524691368724048\n",
      "Epoch 731, Loss: 12.915768123399644, Val-Loss: 14.314810190952299\n",
      "Epoch 732, Loss: 12.902861732457371, Val-Loss: 14.328749263730804\n",
      "Epoch 733, Loss: 12.908242354910188, Val-Loss: 14.226172990501533\n",
      "Epoch 734, Loss: 12.87978311208611, Val-Loss: 14.434935787811282\n",
      "Epoch 735, Loss: 12.908574241406937, Val-Loss: 14.65069473857258\n",
      "Epoch 736, Loss: 12.910989660870628, Val-Loss: 14.572161807169625\n",
      "Epoch 737, Loss: 12.879122429828866, Val-Loss: 14.280393513459769\n",
      "Epoch 738, Loss: 12.908646731976358, Val-Loss: 14.393380263158958\n",
      "Epoch 739, Loss: 12.926598580647301, Val-Loss: 14.472862997188894\n",
      "Epoch 740, Loss: 12.89770677384534, Val-Loss: 14.537892850617073\n",
      "Epoch 741, Loss: 12.895256428722325, Val-Loss: 14.286902798544043\n",
      "Epoch 742, Loss: 12.888803057083974, Val-Loss: 14.62097797554597\n",
      "Epoch 743, Loss: 12.900497247136833, Val-Loss: 14.377633100056867\n",
      "Epoch 744, Loss: 12.877325131450414, Val-Loss: 14.370890135750068\n",
      "Epoch 745, Loss: 12.895366048976557, Val-Loss: 14.521961855009003\n",
      "Epoch 746, Loss: 12.88196343305979, Val-Loss: 14.491517209951942\n",
      "Epoch 747, Loss: 12.881309955495176, Val-Loss: 14.588453002463291\n",
      "Epoch 748, Loss: 12.897218512350388, Val-Loss: 14.415203095278878\n",
      "Epoch 749, Loss: 12.878692076643034, Val-Loss: 14.544789707387599\n",
      "Epoch 750, Loss: 12.886414362252689, Val-Loss: 14.53503535785291\n",
      "Epoch 751, Loss: 12.885793930203837, Val-Loss: 14.666298707278981\n",
      "Epoch 752, Loss: 12.870654526935118, Val-Loss: 14.318942588601226\n",
      "Epoch 753, Loss: 12.88581571351833, Val-Loss: 14.397615842271733\n",
      "Epoch 754, Loss: 12.877048646564837, Val-Loss: 14.556227394212154\n",
      "Epoch 755, Loss: 12.933364706659477, Val-Loss: 14.436963793070424\n",
      "Epoch 756, Loss: 12.880230014963413, Val-Loss: 14.582080701687289\n",
      "Epoch 757, Loss: 12.871996940146381, Val-Loss: 14.480697857540902\n",
      "Epoch 758, Loss: 12.85974620535439, Val-Loss: 14.323320765405011\n",
      "Epoch 759, Loss: 12.87639021049689, Val-Loss: 14.328723379941707\n",
      "Epoch 760, Loss: 12.874862278585681, Val-Loss: 14.569552194928946\n",
      "Epoch 761, Loss: 12.872109100357946, Val-Loss: 14.614619758721826\n",
      "Epoch 762, Loss: 12.869140530346549, Val-Loss: 14.562508824383446\n",
      "Epoch 763, Loss: 12.869075007205355, Val-Loss: 14.570898576415662\n",
      "Epoch 764, Loss: 12.86713138764692, Val-Loss: 14.531146598389304\n",
      "Epoch 765, Loss: 12.885598346161204, Val-Loss: 14.546746149552199\n",
      "Epoch 766, Loss: 12.864812450765514, Val-Loss: 14.543334589434837\n",
      "Epoch 767, Loss: 12.85340940123632, Val-Loss: 14.429339042262345\n",
      "Epoch 768, Loss: 12.870487872707224, Val-Loss: 14.438865658421552\n",
      "Epoch 769, Loss: 12.862611961669938, Val-Loss: 14.590684783904036\n",
      "Epoch 770, Loss: 12.850636644057696, Val-Loss: 14.530496006995628\n",
      "Epoch 771, Loss: 12.848082673513986, Val-Loss: 14.416824522657286\n",
      "Epoch 772, Loss: 12.86290567297234, Val-Loss: 14.774743120612172\n",
      "Epoch 773, Loss: 12.858324276930311, Val-Loss: 14.392015593467647\n",
      "Epoch 774, Loss: 12.857839360479892, Val-Loss: 14.53307683140725\n",
      "Epoch 775, Loss: 12.850301137400937, Val-Loss: 14.52219339078391\n",
      "Epoch 776, Loss: 12.865201006197227, Val-Loss: 14.548619115860927\n",
      "Epoch 777, Loss: 12.832412169430366, Val-Loss: 14.399053777670328\n",
      "Epoch 778, Loss: 12.862508987343716, Val-Loss: 14.34485870884171\n",
      "Epoch 779, Loss: 12.8487035473865, Val-Loss: 14.525065879041458\n",
      "Epoch 780, Loss: 12.84613327052556, Val-Loss: 14.466356227500393\n",
      "Epoch 781, Loss: 12.864817392892803, Val-Loss: 14.673237378560856\n",
      "Epoch 782, Loss: 12.849658065243482, Val-Loss: 14.78527029298539\n",
      "Epoch 783, Loss: 12.853471528203835, Val-Loss: 14.549733901908887\n",
      "Epoch 784, Loss: 12.84656613886453, Val-Loss: 14.609069402732036\n",
      "Epoch 785, Loss: 12.862838603626543, Val-Loss: 14.727858870781871\n",
      "Epoch 786, Loss: 12.848473959106295, Val-Loss: 14.524367273127062\n",
      "Epoch 787, Loss: 12.843411900362407, Val-Loss: 14.6301737911738\n",
      "Epoch 788, Loss: 12.830232640031278, Val-Loss: 14.524291442035128\n",
      "Epoch 789, Loss: 12.842083467707797, Val-Loss: 14.585310407678572\n",
      "Epoch 790, Loss: 12.84471540079912, Val-Loss: 14.443941095025073\n",
      "Epoch 791, Loss: 12.844240348304428, Val-Loss: 14.672266494766122\n",
      "Epoch 792, Loss: 12.841072233023114, Val-Loss: 14.554674872482558\n",
      "Epoch 793, Loss: 12.840889199650972, Val-Loss: 14.40562124416626\n",
      "Epoch 794, Loss: 12.832299547214559, Val-Loss: 14.624515391965673\n",
      "Epoch 795, Loss: 12.834058188122782, Val-Loss: 14.400001484898727\n",
      "Epoch 796, Loss: 12.826843651976205, Val-Loss: 14.449961285954668\n",
      "Epoch 797, Loss: 12.836136235870162, Val-Loss: 14.600467013608355\n",
      "Epoch 798, Loss: 12.835355808234448, Val-Loss: 14.613074758337865\n",
      "Epoch 799, Loss: 12.831984023376924, Val-Loss: 14.78805313934272\n",
      "Epoch 800, Loss: 12.832632217640798, Val-Loss: 14.55165623537331\n",
      "Epoch 801, Loss: 12.82722623868929, Val-Loss: 14.785123194952572\n",
      "Epoch 802, Loss: 12.832198436801677, Val-Loss: 14.470993233848285\n",
      "Epoch 803, Loss: 12.821955951211612, Val-Loss: 14.88407158664857\n",
      "Epoch 804, Loss: 12.81613309944925, Val-Loss: 14.573582138627744\n",
      "Epoch 805, Loss: 12.826944018793576, Val-Loss: 14.73979698910922\n",
      "Epoch 806, Loss: 12.819505775314536, Val-Loss: 14.580842193363443\n",
      "Epoch 807, Loss: 12.81214353963653, Val-Loss: 14.48645468339192\n",
      "Epoch 808, Loss: 12.81769335308561, Val-Loss: 14.492386410497902\n",
      "Epoch 809, Loss: 12.819842216499833, Val-Loss: 14.536861012387105\n",
      "Epoch 810, Loss: 12.822418747785363, Val-Loss: 14.653831389044907\n",
      "Epoch 811, Loss: 12.808386494880923, Val-Loss: 14.735179751363058\n",
      "Epoch 812, Loss: 12.818538562148502, Val-Loss: 14.691072184217653\n",
      "Epoch 813, Loss: 12.815594741534333, Val-Loss: 14.733734044889646\n",
      "Epoch 814, Loss: 12.816471796794755, Val-Loss: 14.717977945111647\n",
      "Epoch 815, Loss: 12.80885510168725, Val-Loss: 14.395206802338485\n",
      "Epoch 816, Loss: 12.827355451596086, Val-Loss: 14.67782293702226\n",
      "Epoch 817, Loss: 12.817061510969815, Val-Loss: 14.682965553947358\n",
      "Epoch 818, Loss: 12.81327505305306, Val-Loss: 14.695208139666741\n",
      "Epoch 819, Loss: 12.82105647700269, Val-Loss: 14.771579553673426\n",
      "Epoch 820, Loss: 12.815810037602251, Val-Loss: 14.620850372049315\n",
      "Epoch 821, Loss: 12.804698725758925, Val-Loss: 14.65805285245905\n",
      "Epoch 822, Loss: 12.809234052954686, Val-Loss: 14.623617782539728\n",
      "Epoch 823, Loss: 12.803485614007615, Val-Loss: 14.512095756095235\n",
      "Epoch 824, Loss: 12.809321335640833, Val-Loss: 14.731268241185317\n",
      "Epoch 825, Loss: 12.806139105996179, Val-Loss: 14.758072353817054\n",
      "Epoch 826, Loss: 12.808449578263371, Val-Loss: 14.602298606712974\n",
      "Epoch 827, Loss: 12.804489076377292, Val-Loss: 14.759443002352594\n",
      "Epoch 828, Loss: 12.80544414845649, Val-Loss: 14.68210719294189\n",
      "Epoch 829, Loss: 12.800461292828079, Val-Loss: 14.927244847609929\n",
      "Epoch 830, Loss: 12.805335404906357, Val-Loss: 14.64650575029064\n",
      "Epoch 831, Loss: 12.796430737781732, Val-Loss: 14.730357890922772\n",
      "Epoch 832, Loss: 12.801626360194602, Val-Loss: 14.77350770332829\n",
      "Epoch 833, Loss: 12.79982063442756, Val-Loss: 14.741731001679744\n",
      "Epoch 834, Loss: 12.798669431013515, Val-Loss: 14.58116310786795\n",
      "Epoch 835, Loss: 12.803452351976995, Val-Loss: 14.776950142167902\n",
      "Epoch 836, Loss: 12.801311660500145, Val-Loss: 14.739642658213763\n",
      "Epoch 837, Loss: 12.798206109401942, Val-Loss: 14.784125665503588\n",
      "Epoch 838, Loss: 12.794812117922731, Val-Loss: 14.757385866348955\n",
      "Epoch 839, Loss: 12.79293823540922, Val-Loss: 14.687398500414156\n",
      "Epoch 840, Loss: 12.793083804369319, Val-Loss: 14.65642832133296\n",
      "Epoch 841, Loss: 12.79930598883742, Val-Loss: 14.769189382549403\n",
      "Epoch 842, Loss: 12.789049498163173, Val-Loss: 14.761134681297195\n",
      "Epoch 843, Loss: 12.790083615119041, Val-Loss: 14.7843014349202\n",
      "Epoch 844, Loss: 12.795941028453928, Val-Loss: 14.615372847447558\n",
      "Epoch 845, Loss: 12.78951109672984, Val-Loss: 14.823319841861856\n",
      "Epoch 846, Loss: 12.792050479724299, Val-Loss: 14.70349768018239\n",
      "Epoch 847, Loss: 12.787631671679613, Val-Loss: 14.822022413461788\n",
      "Epoch 848, Loss: 12.7859920313488, Val-Loss: 14.783280369345578\n",
      "Epoch 849, Loss: 12.788038852708798, Val-Loss: 14.73169583160499\n",
      "Epoch 850, Loss: 12.79104025456028, Val-Loss: 14.706667154396555\n",
      "Epoch 851, Loss: 12.78449338054747, Val-Loss: 14.632222172144777\n",
      "Epoch 852, Loss: 12.783132469804283, Val-Loss: 14.624011988573242\n",
      "Epoch 853, Loss: 12.789560799478306, Val-Loss: 14.687420567154897\n",
      "Epoch 854, Loss: 12.783632945840774, Val-Loss: 14.64568976871678\n",
      "Epoch 855, Loss: 12.778744645626828, Val-Loss: 14.744902980371453\n",
      "Epoch 856, Loss: 12.781544794649726, Val-Loss: 14.745131772607174\n",
      "Epoch 857, Loss: 12.785748669619302, Val-Loss: 14.769459808350817\n",
      "Epoch 858, Loss: 12.77865405353217, Val-Loss: 14.681147156168603\n",
      "Epoch 859, Loss: 12.774696873875087, Val-Loss: 14.796613447983601\n",
      "Epoch 860, Loss: 12.78326494886604, Val-Loss: 14.785051244684151\n",
      "Epoch 861, Loss: 12.781800608322376, Val-Loss: 14.82680079067093\n",
      "Epoch 862, Loss: 12.77767694524946, Val-Loss: 14.770377359684693\n",
      "Epoch 863, Loss: 12.763256741033365, Val-Loss: 14.699928199208228\n",
      "Epoch 864, Loss: 12.779080728323974, Val-Loss: 14.800925415407352\n",
      "Epoch 865, Loss: 12.776609333451352, Val-Loss: 14.778690309044144\n",
      "Epoch 866, Loss: 12.773536099012547, Val-Loss: 14.777283559873709\n",
      "Epoch 867, Loss: 12.773572505962424, Val-Loss: 14.786420053226395\n",
      "Epoch 868, Loss: 12.778144100107928, Val-Loss: 14.711119527400946\n",
      "Epoch 869, Loss: 12.776010541105503, Val-Loss: 14.68388607194018\n",
      "Epoch 870, Loss: 12.774386298611468, Val-Loss: 14.737733352198651\n",
      "Epoch 871, Loss: 12.77073002525313, Val-Loss: 14.75819472670834\n",
      "Epoch 872, Loss: 12.769646967378913, Val-Loss: 14.88362720282508\n",
      "Epoch 873, Loss: 12.773880048540647, Val-Loss: 14.879980140415995\n",
      "Epoch 874, Loss: 12.769241881127057, Val-Loss: 14.853405089245332\n",
      "Epoch 875, Loss: 12.772018869108512, Val-Loss: 14.83442391955485\n",
      "Epoch 876, Loss: 12.763101225210944, Val-Loss: 14.904764614192315\n",
      "Epoch 877, Loss: 12.768220098370955, Val-Loss: 14.84971542827104\n",
      "Epoch 878, Loss: 12.766812309792277, Val-Loss: 14.800450136765221\n",
      "Epoch 879, Loss: 12.765819658288173, Val-Loss: 14.753072145408412\n",
      "Epoch 880, Loss: 12.762524712720063, Val-Loss: 14.817790717682795\n",
      "Epoch 881, Loss: 12.768595192650274, Val-Loss: 14.822382678717819\n",
      "Epoch 882, Loss: 12.765266434644909, Val-Loss: 14.874799481246555\n",
      "Epoch 883, Loss: 12.765324774876987, Val-Loss: 14.813279748743144\n",
      "Epoch 884, Loss: 12.765880544451273, Val-Loss: 14.851357201643992\n",
      "Epoch 885, Loss: 12.761045928172885, Val-Loss: 14.856391197737693\n",
      "Epoch 886, Loss: 12.764725182220124, Val-Loss: 14.822752740015488\n",
      "Epoch 887, Loss: 12.76364875206485, Val-Loss: 14.899365089432807\n",
      "Epoch 888, Loss: 12.765368296011948, Val-Loss: 14.866877657361893\n",
      "Epoch 889, Loss: 12.762925462002467, Val-Loss: 14.851544994984692\n",
      "Epoch 890, Loss: 12.75952251338015, Val-Loss: 14.776933348029157\n",
      "Epoch 891, Loss: 12.761563201704709, Val-Loss: 14.848493190889485\n",
      "Epoch 892, Loss: 12.761420351444883, Val-Loss: 14.847025332354637\n",
      "Epoch 893, Loss: 12.759935322999217, Val-Loss: 14.863598292267936\n",
      "Epoch 894, Loss: 12.758901692093987, Val-Loss: 14.844514277224441\n",
      "Epoch 895, Loss: 12.758617713810334, Val-Loss: 14.76939160075854\n",
      "Epoch 896, Loss: 12.758210678190064, Val-Loss: 14.87085677627475\n",
      "Epoch 897, Loss: 12.754196689850483, Val-Loss: 14.808457620088548\n",
      "Epoch 898, Loss: 12.755059712220914, Val-Loss: 14.833887455798488\n",
      "Epoch 899, Loss: 12.757504926334093, Val-Loss: 14.839948957591469\n",
      "Epoch 900, Loss: 12.75347719341995, Val-Loss: 14.82169091737702\n",
      "Epoch 901, Loss: 12.75582182059896, Val-Loss: 14.847964360332934\n",
      "Epoch 902, Loss: 12.7538489281263, Val-Loss: 14.869469172912224\n",
      "Epoch 903, Loss: 12.756664151386358, Val-Loss: 14.823145067292154\n",
      "Epoch 904, Loss: 12.751065545982557, Val-Loss: 14.850145139766692\n",
      "Epoch 905, Loss: 12.754028874895116, Val-Loss: 14.80432190769861\n",
      "Epoch 906, Loss: 12.75360311006575, Val-Loss: 14.826596288761015\n",
      "Epoch 907, Loss: 12.753871131639054, Val-Loss: 14.801661313405273\n",
      "Epoch 908, Loss: 12.752274542755815, Val-Loss: 14.830125101819918\n",
      "Epoch 909, Loss: 12.750758706943229, Val-Loss: 14.838816601248121\n",
      "Epoch 910, Loss: 12.751846541160894, Val-Loss: 14.87226016448518\n",
      "Epoch 911, Loss: 12.750500571098785, Val-Loss: 14.809950868383753\n",
      "Epoch 912, Loss: 12.748606618077062, Val-Loss: 14.852513137215928\n",
      "Epoch 913, Loss: 12.752362600683771, Val-Loss: 14.831240406646627\n",
      "Epoch 914, Loss: 12.749929650465207, Val-Loss: 14.886440778486554\n",
      "Epoch 915, Loss: 12.7506729148867, Val-Loss: 14.87435976802918\n",
      "Epoch 916, Loss: 12.74709009695271, Val-Loss: 14.849113561796852\n",
      "Epoch 917, Loss: 12.74871218929306, Val-Loss: 14.85933497189949\n",
      "Epoch 918, Loss: 12.748680735624706, Val-Loss: 14.884906951165203\n",
      "Epoch 919, Loss: 12.748231921606065, Val-Loss: 14.873891268992317\n",
      "Epoch 920, Loss: 12.746672406175119, Val-Loss: 14.88293421503635\n",
      "Epoch 921, Loss: 12.746199784566691, Val-Loss: 14.878484682702354\n",
      "Epoch 922, Loss: 12.746622177011265, Val-Loss: 14.881753122449494\n",
      "Epoch 923, Loss: 12.746801280323847, Val-Loss: 14.875383606439456\n",
      "Epoch 924, Loss: 12.745256603123618, Val-Loss: 14.877858392769825\n",
      "Epoch 925, Loss: 12.745822337127873, Val-Loss: 14.875188020263693\n",
      "Epoch 926, Loss: 12.74503736022249, Val-Loss: 14.8871200780727\n",
      "Epoch 927, Loss: 12.744388486750395, Val-Loss: 14.87037112284796\n",
      "Epoch 928, Loss: 12.744918320017826, Val-Loss: 14.85835366078784\n",
      "Epoch 929, Loss: 12.745144126806922, Val-Loss: 14.868628208178837\n",
      "Epoch 930, Loss: 12.743479063168127, Val-Loss: 14.871517643933721\n",
      "Epoch 931, Loss: 12.743771362967276, Val-Loss: 14.878752823124797\n",
      "Epoch 932, Loss: 12.742774723645004, Val-Loss: 14.873655885650525\n",
      "Epoch 933, Loss: 12.742381260281544, Val-Loss: 14.88971910025235\n",
      "Epoch 934, Loss: 12.741474000794934, Val-Loss: 14.874102868501414\n",
      "Epoch 935, Loss: 12.743036715352417, Val-Loss: 14.886094311904548\n",
      "Epoch 936, Loss: 12.741123260533637, Val-Loss: 14.895538990254543\n",
      "Epoch 937, Loss: 12.742147113351505, Val-Loss: 14.904930101513195\n",
      "Epoch 938, Loss: 12.741886442232126, Val-Loss: 14.909450821398753\n",
      "Epoch 939, Loss: 12.740917820357915, Val-Loss: 14.890835637204436\n",
      "Epoch 940, Loss: 12.740172067996301, Val-Loss: 14.886557193432159\n",
      "Epoch 941, Loss: 12.739050631833711, Val-Loss: 14.880539429012417\n",
      "Epoch 942, Loss: 12.73992348817208, Val-Loss: 14.885032762277644\n",
      "Epoch 943, Loss: 12.740609835840422, Val-Loss: 14.879188425125886\n",
      "Epoch 944, Loss: 12.738851780232737, Val-Loss: 14.877978498864028\n",
      "Epoch 945, Loss: 12.739017658101035, Val-Loss: 14.884398874422544\n",
      "Epoch 946, Loss: 12.738816682289498, Val-Loss: 14.876816585676675\n",
      "Epoch 947, Loss: 12.738040102147767, Val-Loss: 14.87854828675709\n",
      "Epoch 948, Loss: 12.738200810955727, Val-Loss: 14.887382985943969\n",
      "Epoch 949, Loss: 12.7382498930327, Val-Loss: 14.883674606265618\n",
      "Epoch 950, Loss: 12.737955542357456, Val-Loss: 14.882963438134976\n",
      "Epoch 951, Loss: 12.737202341289905, Val-Loss: 14.882550629106376\n",
      "Epoch 952, Loss: 12.737096658642335, Val-Loss: 14.879598517057513\n",
      "Epoch 953, Loss: 12.736959595562864, Val-Loss: 14.880475309347373\n",
      "Epoch 954, Loss: 12.736357694822008, Val-Loss: 14.880225803240503\n",
      "Epoch 955, Loss: 12.736243602549438, Val-Loss: 14.881481234832444\n",
      "Epoch 956, Loss: 12.736156052920593, Val-Loss: 14.885479978445238\n",
      "Epoch 957, Loss: 12.735687288238983, Val-Loss: 14.87783321200235\n",
      "Epoch 958, Loss: 12.73569479393581, Val-Loss: 14.882958772068868\n",
      "Epoch 959, Loss: 12.735365069749236, Val-Loss: 14.879417001622535\n",
      "Epoch 960, Loss: 12.735172128493273, Val-Loss: 14.878317791224156\n",
      "Epoch 961, Loss: 12.735133178100174, Val-Loss: 14.884350991938163\n",
      "Epoch 962, Loss: 12.73490037927014, Val-Loss: 14.884526920998224\n",
      "Epoch 963, Loss: 12.734689745343715, Val-Loss: 14.877945097638552\n",
      "Epoch 964, Loss: 12.73432203864138, Val-Loss: 14.880035141066676\n",
      "Epoch 965, Loss: 12.734126896291718, Val-Loss: 14.87783255435682\n",
      "Epoch 966, Loss: 12.734138214280138, Val-Loss: 14.877318101298066\n",
      "Epoch 967, Loss: 12.73387606927731, Val-Loss: 14.877170853521108\n",
      "Epoch 968, Loss: 12.733597422415704, Val-Loss: 14.875269509027916\n",
      "Epoch 969, Loss: 12.733578928608292, Val-Loss: 14.876303468799746\n",
      "Epoch 970, Loss: 12.733433372309946, Val-Loss: 14.878644148185389\n",
      "Epoch 971, Loss: 12.733411635676497, Val-Loss: 14.874783762326038\n",
      "Epoch 972, Loss: 12.733173462043824, Val-Loss: 14.877203850372426\n",
      "Epoch 973, Loss: 12.732920051890357, Val-Loss: 14.876826719035112\n",
      "Epoch 974, Loss: 12.732945542170212, Val-Loss: 14.877565064211462\n",
      "Epoch 975, Loss: 12.73288725636184, Val-Loss: 14.877262806851876\n",
      "Epoch 976, Loss: 12.732602056255907, Val-Loss: 14.876830122791507\n",
      "Epoch 977, Loss: 12.732556628671755, Val-Loss: 14.876176335723244\n",
      "Epoch 978, Loss: 12.732317544263038, Val-Loss: 14.874620800896512\n",
      "Epoch 979, Loss: 12.732274761239513, Val-Loss: 14.874594359198461\n",
      "Epoch 980, Loss: 12.73216890799839, Val-Loss: 14.874378032709648\n",
      "Epoch 981, Loss: 12.732071206956373, Val-Loss: 14.874762441026741\n",
      "Epoch 982, Loss: 12.731978810510883, Val-Loss: 14.874998561054918\n",
      "Epoch 983, Loss: 12.731871566466918, Val-Loss: 14.875103864529343\n",
      "Epoch 984, Loss: 12.731786592636, Val-Loss: 14.875119358780893\n",
      "Epoch 985, Loss: 12.731720256501175, Val-Loss: 14.874651968487157\n",
      "Epoch 986, Loss: 12.731641238384729, Val-Loss: 14.87511337066332\n",
      "Epoch 987, Loss: 12.731554358324777, Val-Loss: 14.875027007231717\n",
      "Epoch 988, Loss: 12.731517638989269, Val-Loss: 14.874867439179814\n",
      "Epoch 989, Loss: 12.731464006227116, Val-Loss: 14.87515299927625\n",
      "Epoch 990, Loss: 12.731371075518929, Val-Loss: 14.875170221505751\n",
      "Epoch 991, Loss: 12.731317960906885, Val-Loss: 14.875213250977023\n",
      "Epoch 992, Loss: 12.73129680963256, Val-Loss: 14.874996747179875\n",
      "Epoch 993, Loss: 12.731254081656317, Val-Loss: 14.875088318824613\n",
      "Epoch 994, Loss: 12.73121925571442, Val-Loss: 14.874974852687432\n",
      "Epoch 995, Loss: 12.731189584303953, Val-Loss: 14.874710458526517\n",
      "Epoch 996, Loss: 12.731157356114714, Val-Loss: 14.874756363525057\n",
      "Epoch 997, Loss: 12.731143660741235, Val-Loss: 14.874672500270004\n",
      "Epoch 998, Loss: 12.73113411112831, Val-Loss: 14.87465708475106\n",
      "Epoch 999, Loss: 12.731127817681916, Val-Loss: 14.874645947687352\n",
      "Epoch 1000, Loss: 12.731126590856723, Val-Loss: 14.8746457846409\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.swa_utils import AveragedModel#, get_ema_multi_avg_fn\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = Net()\n",
    "\n",
    "decay = 0.995\n",
    "#averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=0.0001, \n",
    "                                                steps_per_epoch=len(train_loader), \n",
    "                                                epochs=num_epochs, \n",
    "                                                pct_start=0.1,\n",
    "                                                final_div_factor=1e4)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "#                                                        T_max=50, \n",
    "#                                                        eta_min=0, \n",
    "#                                                        last_epoch=-1, \n",
    "#                                                        verbose='deprecated')\n",
    "\n",
    "train_writer = SummaryWriter()\n",
    "val_writer = SummaryWriter()\n",
    "\n",
    "dataloader = train_loader\n",
    "val_loader = val_loader\n",
    "\n",
    "# dataloader = single_loader\n",
    "# val_loader = single_val_loader\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "print(\"start training...\")\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        etl_features = batch['etl_features'].squeeze(0)\n",
    "        htl_features = batch['htl_features'].squeeze(0)\n",
    "        etl_edge_indices = batch['etl_edge_indices'].squeeze(0)\n",
    "        htl_edge_indices = batch['htl_edge_indices'].squeeze(0)\n",
    "        absorber = batch['absorber']\n",
    "        bandgap = batch['bandgap']\n",
    "        # etl_encoded = batch['etl_encoded'] # only for label_encoding model\n",
    "        # htl_encoded = batch['htl_encoded'] # only for label_encoding model\n",
    "        \n",
    "        out = model(etl_features, \n",
    "                    htl_features, \n",
    "                    etl_edge_indices, \n",
    "                    htl_edge_indices, \n",
    "                    absorber,\n",
    "                    bandgap,\n",
    "                    # etl_encoded, # only for label_encoding model\n",
    "                    # htl_encoded # only for label_encoding model\n",
    "                   )\n",
    " \n",
    "        loss = criterion(out, batch['pce'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #averaged_model.update_parameters(model)\n",
    "        scheduler.step()\n",
    "        total_loss += float(loss)\n",
    "    train_writer.add_scalar('Training Loss', float(loss), epoch)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in val_loader:        \n",
    "        etl_features = batch['etl_features'].squeeze(0)\n",
    "        htl_features = batch['htl_features'].squeeze(0)\n",
    "        etl_edge_indices = batch['etl_edge_indices'].squeeze(0)\n",
    "        htl_edge_indices = batch['htl_edge_indices'].squeeze(0)\n",
    "        absorber = batch['absorber']\n",
    "        bandgap = batch['bandgap']\n",
    "        # etl_encoded = batch['etl_encoded'] # only for label_encoding model\n",
    "        # htl_encoded = batch['htl_encoded'] # only for label_encoding model\n",
    "        out = model(etl_features, \n",
    "                    htl_features, \n",
    "                    etl_edge_indices, \n",
    "                    htl_edge_indices, \n",
    "                    absorber,\n",
    "                    bandgap,\n",
    "                    # etl_encoded, # only for label_encoding model\n",
    "                    # htl_encoded # only for label_encoding model\n",
    "                   ) \n",
    "        loss = criterion(out, batch['pce'])\n",
    "        total_val_loss += float(loss)\n",
    "    val_writer.add_scalar('Validation Loss', float(loss), epoch)\n",
    "\n",
    "    if total_val_loss <= best_validation_loss:\n",
    "        best_validation_loss = total_val_loss\n",
    "        torch.save(model.state_dict(), 'models/GNN_12_absorber_only_32.pth')\n",
    "        #torch.save(averaged_model.state_dict(), 'models/GNN_29_avg_model.pth')\n",
    "        print(f'(new best)')\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}, Val-Loss: {total_val_loss/len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac3e2bf-2bfe-4417-bc3d-0960671a3be5",
   "metadata": {},
   "source": [
    "# Evaluation and model comparison\n",
    "\n",
    "For predictions and loading the models, the model architecture needs to be stated again. \n",
    "Adjust the above model according to the following information and then proceed to the plot section.\n",
    "\n",
    "## Model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add105a7-0d0a-4c1e-ada1-8a363afad3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GNN_1\n",
    "in 1000 epochs to loss 13.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8e6179e4-678f-4f16-88ca-ec448879c323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### insert into GNN definition section ####################\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 16\n",
    "        self.htl_embedding_dimensions = 16\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        self.etl_mpnn = GCNConv(in_channels=1, out_channels=self.etl_embedding_dimensions)\n",
    "        self.htl_mpnn = GCNConv(in_channels=1, out_channels=self.htl_embedding_dimensions)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension, self.hidden_dimension)\n",
    "        \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear(self.hidden_dimension,  self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):\n",
    "        etl_x = self.etl_mpnn(etl_features, etl_edge_indices)\n",
    "        etl_x = global_mean_pool(etl_x, torch.zeros(etl_x.size(0), dtype=torch.long))\n",
    "        \n",
    "        htl_x = self.htl_mpnn(htl_features, htl_edge_indices)\n",
    "        htl_x = global_mean_pool(htl_x, torch.zeros(htl_x.size(0), dtype=torch.long))\n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        #x = F.softplus(x) # possibly no softplus here removes the error\n",
    "        return x\n",
    "\n",
    "####### end of insert #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2607bdb-3fa3-4070-9652-26085672ab19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### insert into Training section ##############\n",
    "\n",
    "decay = 0.995\n",
    "averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=num_epochs, pct_start=0.1,final_div_factor=1e2)\n",
    "\n",
    "######### end of insert ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffec5be-4584-4924-944f-91582c60a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GNN_2\n",
    "Everything as in GNN_1, but atom weight was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c82ac-bbe5-44d2-b96f-8542c33ad2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in smiles_to_graph insert:\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atomic_num = atom.GetAtomicNum()\n",
    "        atomic_weight = atom.GetMass()\n",
    "        atom_features.append([atomic_num, atomic_weight])\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d62e7-8766-49ac-8a63-afd420125694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the default data in creating MolecularDataset class, change dimensions of the zero verctor:\n",
    "default_data = {'x': torch.zeros((1,2),  dtype=torch.float), 'edge_index': torch.zeros((2,1), dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90375235-4086-49a2-afe9-91bd7f834f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the net definitions, change input channels to 2\n",
    "        self.etl_mpnn = GCNConv(in_channels=2, out_channels=self.etl_embedding_dimensions)\n",
    "        self.htl_mpnn = GCNConv(in_channels=2, out_channels=self.htl_embedding_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895c974-329b-4d50-92a5-43b44d7c88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GNN_3\n",
    "Mean Squared Error: 14.77943229675293\n",
    "R^2 Score: 0.5403929552966777\n",
    "Mean Absolute Error: 2.8730850219726562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaac4b1-426b-4dcc-8413-acfec924b683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace in smiles_to_graph\n",
    "    def get_atom_features(atom):\n",
    "        features = [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetMass(),\n",
    "            atom.GetExplicitValence(),\n",
    "            atom.GetFormalCharge(),\n",
    "            atom.GetHybridization(),\n",
    "            atom.GetIsAromatic(),\n",
    "            atom.GetNumRadicalElectrons(),\n",
    "            atom.GetTotalValence(),\n",
    "            atom.IsInRing()\n",
    "        ]\n",
    "        return [float(f) if not isinstance(f, bool) else int(f) for f in features]\n",
    "\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(get_atom_features(atom))\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c91af-46d2-421c-9d1e-1b02eba0018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the default data in creating MolecularDataset class, change dimensions of the zero verctor:\n",
    "default_data = {'x': torch.zeros((1,9),  dtype=torch.float), 'edge_index': torch.zeros((2,1), dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30072c-4685-4f04-af01-b7bffe7d1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the net definitions, change input channels to 9\n",
    "        self.etl_mpnn = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions)\n",
    "        self.htl_mpnn = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e47deb-adb9-40b0-a5f1-b358acfa0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_4\n",
    "Mean Squared Error: 15.379298210144043\n",
    "R^2 Score: 0.5217384699304368\n",
    "Mean Absolute Error: 2.8592941761016846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f03a4-e380-4ad1-91a3-7aff902d8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything as before, but embedding dimensions 32 instead of 16.\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65842276-0b9e-4766-9894-a73c7d5b9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in scheduler final_div_factor: 1e3 (because 1e2 overfits while still jumping losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f5fd6-ee7a-4fe4-b497-ffad5887bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_5\n",
    "\n",
    "Eliminate the graph information by replacing them with zeros. Everything else the same as in GNN_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8eaf41-916b-4f3d-96ba-9d2fd1fa81bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert this instead of the forward function in the GNN_4 Net().\n",
    "\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):\n",
    "        # etl_x = self.etl_mpnn(etl_features, etl_edge_indices) # comment this out for no-graph prediction\n",
    "        # etl_x = self.etl_mpnn(etl_x, etl_edge_indices)\n",
    "        # etl_x = global_mean_pool(etl_x, torch.zeros(etl_x.size(0), dtype=torch.long)) # comment this out for no-graph prediction\n",
    "        \n",
    "        # htl_x = self.htl_mpnn(htl_features, htl_edge_indices) # comment this out for no-graph prediction\n",
    "        # htl_x = self.htl_mpnn(htl_x, htl_edge_indices)\n",
    "        # htl_x = global_mean_pool(htl_x, torch.zeros(htl_x.size(0), dtype=torch.long)) # comment this out for no-graph prediction\n",
    "        \n",
    "        etl_x = torch.zeros([1,self.etl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        htl_x = torch.zeros([1,self.htl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "\n",
    "        # absorbers_embed = self.fc_absorber(absorbers)\n",
    "        # x = torch.cat([etl_x, htl_x, absorbers_embed], dim=1) \n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        x = F.softplus(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e96518-70aa-4b08-a0c2-b7de10afec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_6\n",
    "max_lr=0.0001\n",
    "final_div_factor=1e2\n",
    "\n",
    "Mean Squared Error: 13.848991394042969\n",
    "R^2 Score: 0.5693276002514303\n",
    "Mean Absolute Error: 2.7927744388580322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c89b89-8875-4b06-97de-510866de61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_7\n",
    "everything as GNN_6 but final_div_factor = 1e3\n",
    "\n",
    "Mean Squared Error: 13.550704002380371\n",
    "R^2 Score: 0.5786036347103873\n",
    "Mean Absolute Error: 2.7916390895843506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e6d96-4501-4240-af0a-014ed8d91232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_8\n",
    "everything as GNN_6 and GNN_7 but final_div_factor = 1e4\n",
    "Mean Squared Error: 13.562051773071289\n",
    "R^2 Score: 0.5782507841432195\n",
    "Mean Absolute Error: 2.742356300354004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef87676-23d1-49ee-9f5c-27fd7d8d2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_9_test\n",
    "final_div_factor = 1e4\n",
    "eliminate PCEs below 2 %\n",
    "Mean Squared Error: 12.327483177185059\n",
    "R^2 Score: 0.4462597339252242\n",
    "Mean Absolute Error: 2.718646764755249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db40d6-bff2-4e85-92a9-7c8490a845b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_10\n",
    "no PCEs below 2%\n",
    "final_div_factor = 1e4\n",
    "weight_decay=1e-3 (before: 1e-4)\n",
    "\n",
    "Mean Squared Error: 12.734729766845703\n",
    "R^2 Score: 0.4279665440353737\n",
    "Mean Absolute Error: 2.750393867492676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc99d8-6533-471d-a56c-408eb7d71aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_11\n",
    "everything as GNN_10 but weight decay 1e-5\n",
    "\n",
    "Mean Squared Error: 12.17917251586914\n",
    "R^2 Score: 0.45292170065419457\n",
    "Mean Absolute Error: 2.6727020740509033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44765cdf-acd3-4290-b4dd-c336a52b7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_12\n",
    "weight decay back to 1e-4\n",
    "added an additional layer for htl and etl processing\n",
    "no softplus layer in the end\n",
    "\n",
    "Mean Squared Error: 11.764592170715332\n",
    "R^2 Score: 0.47154431620395776\n",
    "Mean Absolute Error: 2.6425602436065674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae48ec33-b98f-471e-be14-f6e571f5bdef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, MLP, global_add_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.etl_embedding_dimensions = 32\n",
    "        self.htl_embedding_dimensions = 32\n",
    "        self.absorber_dimensions = 76\n",
    "        self.bandgap_dimension = 1\n",
    "        self.hidden_dimension = 32\n",
    "        self.number_of_regression_layers = 3\n",
    "\n",
    "        # Two GCN layers for ETL processing\n",
    "        self.etl_mpnn1 = GCNConv(in_channels=9, out_channels=self.etl_embedding_dimensions)\n",
    "        self.etl_mpnn2 = GCNConv(in_channels=self.etl_embedding_dimensions, out_channels=self.etl_embedding_dimensions)\n",
    "\n",
    "        # Two GCN layers for HTL processing\n",
    "        self.htl_mpnn1 = GCNConv(in_channels=9, out_channels=self.htl_embedding_dimensions)\n",
    "        self.htl_mpnn2 = GCNConv(in_channels=self.htl_embedding_dimensions, out_channels=self.htl_embedding_dimensions)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.etl_embedding_dimensions + self.htl_embedding_dimensions + self.absorber_dimensions + self.bandgap_dimension, self.hidden_dimension)\n",
    "    \n",
    "        self.regression_layers = torch.nn.ModuleList([self.fc1])\n",
    "        self.regression_layers.extend([torch.nn.Linear(self.hidden_dimension, self.hidden_dimension) for i in range(1, self.number_of_regression_layers-1)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(self.hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):\n",
    "        # Two-layer processing for ETL\n",
    "        etl_x = self.etl_mpnn1(etl_features, etl_edge_indices)\n",
    "        etl_x = F.relu(etl_x)  # Add activation after the first layer\n",
    "        etl_x = self.etl_mpnn2(etl_x, etl_edge_indices)\n",
    "        etl_x = global_mean_pool(etl_x, torch.zeros(etl_x.size(0), dtype=torch.long))\n",
    "    \n",
    "        # Two-layer processing for HTL\n",
    "        htl_x = self.htl_mpnn1(htl_features, htl_edge_indices)\n",
    "        htl_x = F.relu(htl_x)  # Add activation after the first layer\n",
    "        htl_x = self.htl_mpnn2(htl_x, htl_edge_indices)\n",
    "        htl_x = global_mean_pool(htl_x, torch.zeros(htl_x.size(0), dtype=torch.long))\n",
    "    \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers:\n",
    "            x = F.leaky_relu(layer(x))\n",
    "    \n",
    "        x = self.fc_out(x)\n",
    "        #x = F.softplus(x) # possibly no softplus here removes the error\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2447d-b23d-4182-b3d3-aa6145377b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_13\n",
    "everything as GNN_12 but with softplus layer in the end\n",
    "\n",
    "Mean Squared Error: 12.195013999938965\n",
    "R^2 Score: 0.4522101290976822\n",
    "Mean Absolute Error: 2.717991352081299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e265c30-ee35-43c2-b94e-36d15a95c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_14\n",
    "everything as GNN_12 but leaky_relu between the two etl and htl layers instead of relu\n",
    "--> overfits very badly after 400 epochs\n",
    "\n",
    "Mean Squared Error: 12.911194801330566\n",
    "R^2 Score: 0.4200398814332983\n",
    "Mean Absolute Error: 2.805245876312256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6bab4-1ec6-41be-a1f6-7da2f5754dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_15\n",
    "max_lr=0.001 instead of 0.0001, everything else as GNN_12\n",
    "Mean Squared Error: 12.947954177856445\n",
    "R^2 Score: 0.4183886945076104\n",
    "Mean Absolute Error: 2.810683012008667\n",
    "\n",
    "Was overwritten by what was supposed to become GNN_16\n",
    "max_lr back to 0.0001, pct_start = 0.2 instead of 0.1\n",
    "\n",
    "Mean Squared Error: 12.454482078552246\n",
    "R^2 Score: 0.4405550240903354\n",
    "Mean Absolute Error: 2.768221378326416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc93f9-f328-4671-a882-3967a81c3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_16\n",
    "Mean Squared Error: 13.04375171661377\n",
    "R^2 Score: 0.4140855604161916\n",
    "Mean Absolute Error: 2.7799715995788574\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c0400-83ed-498d-b073-e1c8c36fac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN_17\n",
    "trying to rebuild GNN_16\n",
    "\n",
    "Mean Squared Error: 12.994876861572266\n",
    "R^2 Score: 0.4162809750824753\n",
    "Mean Absolute Error: 2.7726638317108154\n",
    "\n",
    "This is not GNN_12..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502ac5b-75f9-4a40-a791-d4ceefd4cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_18\n",
    "final_div_factor back to 1e3 hoping to find the GNN_12 configuration again\n",
    "\n",
    "Mean Squared Error: 12.174577713012695\n",
    "R^2 Score: 0.4531281193692762\n",
    "Mean Absolute Error: 2.6740314960479736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08eae7-169a-4196-a184-d9d63ee3cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_19\n",
    "lr = 1-e3\n",
    "\n",
    "Mean Squared Error: 12.212804794311523\n",
    "R^2 Score: 0.45141097582191614\n",
    "Mean Absolute Error: 2.671917676925659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724f13c-5bcd-40b5-a2e0-28179721d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_20\n",
    "max_lr = 0.00001\n",
    "\n",
    "Mean Squared Error: 14.260919570922852\n",
    "R^2 Score: 0.359411411740551\n",
    "Mean Absolute Error: 2.969792127609253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd3a8b-9392-49ee-806e-74a4a9959c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_21\n",
    "max_lr = 0.001\n",
    "\n",
    "Mean Squared Error: 12.907291412353516\n",
    "R^2 Score: 0.42021520018931935\n",
    "Mean Absolute Error: 2.872129440307617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a1dec-92c2-495b-8c4f-a20c0620935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_22\n",
    "cycle annealing scheduler with T_max = 100\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                       T_max=100, \n",
    "                                                       eta_min=0, \n",
    "                                                       last_epoch=-1, \n",
    "                                                       verbose='deprecated')\n",
    "\n",
    "\n",
    "Mean Squared Error: 12.652148246765137\n",
    "R^2 Score: 0.43167606946355797\n",
    "Mean Absolute Error: 2.7812609672546387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cbe6a-abe8-42dc-91af-b8dc7ec3b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_23\n",
    "same as GNN_22 but with lr = 1e-4\n",
    "\n",
    "Mean Squared Error: 12.442548751831055\n",
    "R^2 Score: 0.44109108444848644\n",
    "Mean Absolute Error: 2.750389337539673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ec96f-5c49-4f23-9e16-16e33cfde183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_24\n",
    "same as GNN_23 but with t_max = 50\n",
    "\n",
    "Mean Squared Error: 13.888327598571777\n",
    "R^2 Score: 0.37614787548216977\n",
    "Mean Absolute Error: 2.7717573642730713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006defc2-8e2c-49a9-bea9-2b4b9f353102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_25\n",
    "go back to OneCycleR with smaller peak lr\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=0.0001, \n",
    "                                                steps_per_epoch=len(train_loader), \n",
    "                                                epochs=num_epochs, \n",
    "                                                pct_start=0.1,\n",
    "                                                final_div_factor\n",
    "                                                \n",
    "Mean Squared Error: 12.103560447692871\n",
    "R^2 Score: 0.45631812300883134\n",
    "Mean Absolute Error: 2.6630618572235107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83da426-88bd-4b16-b26c-78cf892c6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_26\n",
    "as 25, but with also max_lr = 0.00001\n",
    "\n",
    "Mean Squared Error: 14.209254264831543\n",
    "R^2 Score: 0.3617321728196772\n",
    "Mean Absolute Error: 2.95304799079895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778f893-3238-4e8e-a71e-642899b9c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_27\n",
    "max_lr back to 0.0001\n",
    "\n",
    "Number of regression layers = 2\n",
    "\n",
    "Mean Squared Error: 13.370475769042969\n",
    "R^2 Score: 0.3994093672942369\n",
    "Mean Absolute Error: 2.8325576782226562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339448c-6efb-4ffc-9051-83dcafaa3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_28\n",
    "hidden_dimensions = 64 instead of 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11c3ed-fd7d-41f3-9dc0-c118c615d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON MODELS\n",
    "\n",
    "## GNN_12\n",
    "# --> load as stated above\n",
    "Mean Squared Error: 10.126177787780762\n",
    "R^2 Score: 0.5662597949761423\n",
    "Mean Absolute Error: 2.5018486976623535\n",
    "\n",
    "## GNN_12_absorber_only\n",
    "# set the graphs to zero\n",
    "    def forward(self, etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorbers, bandgap):        \n",
    "        etl_x = torch.zeros([1,self.etl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        htl_x = torch.zeros([1,self.htl_embedding_dimensions]) # comment this in for no-graph prediction\n",
    "        \n",
    "        x = torch.cat([etl_x, htl_x, absorbers, bandgap], dim=1)\n",
    "        for layer in self.regression_layers: \n",
    "            x = F.leaky_relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "Mean Squared Error: 13.487946510314941\n",
    "R^2 Score: 0.422263328884837\n",
    "Mean Absolute Error: 2.925929069519043\n",
    "\n",
    "## GNN_absorber and label encoded CTLs\n",
    "# Throughout the entire code, the label_encoded etls and htls need to be included\n",
    "# the code is commented out but existent in the code here\n",
    "\n",
    "Mean Squared Error: 14.051036834716797\n",
    "R^2 Score: 0.39814417023079995\n",
    "Mean Absolute Error: 2.8378653526306152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e5970-8f0d-471d-b5e9-a7322a8575c3",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Before this section, you need to have reconstructed the Net() definition used in training. Also, the data for the nodes needs to include the correct amount of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f05c0b66-dcdf-460e-8e9c-a6723e8413ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (etl_mpnn1): GCNConv(9, 32)\n",
       "  (etl_mpnn2): GCNConv(32, 32)\n",
       "  (htl_mpnn1): GCNConv(9, 32)\n",
       "  (htl_mpnn2): GCNConv(32, 32)\n",
       "  (fc1): Linear(in_features=141, out_features=32, bias=True)\n",
       "  (regression_layers): ModuleList(\n",
       "    (0): Linear(in_features=141, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('models/GNN_12.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65d80c9a-42b0-4d7f-a371-dabb9bd24610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 10.126177787780762\n",
      "R^2 Score: 0.5662597949761423\n",
      "Mean Absolute Error: 2.5018486976623535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    for batch in test_loader:\n",
    "        etl_features = batch['etl_features'].squeeze(0)\n",
    "        htl_features = batch['htl_features'].squeeze(0)\n",
    "        etl_edge_indices = batch['etl_edge_indices'].squeeze(0)\n",
    "        htl_edge_indices = batch['htl_edge_indices'].squeeze(0)\n",
    "        absorber = batch['absorber']\n",
    "        bandgap = batch['bandgap']\n",
    "        #etl_encoded = batch['etl_encoded'] # label_encoding\n",
    "        #htl_encoded = batch['htl_encoded'] # label_encoding\n",
    "        true_labels = batch['pce']\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorber, bandgap)\n",
    "\n",
    "        # Forward pass for label encoding    \n",
    "        #predictions = model(etl_features, htl_features, etl_edge_indices, htl_edge_indices, absorber, bandgap, etl_encoded, htl_encoded)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "# Compute evaluation metrics (e.g., Mean Squared Error)\n",
    "mse = mean_squared_error(all_labels, all_predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# You can also compute other metrics like R^2 score, MAE, etc.\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "r2 = r2_score(all_labels, all_predictions)\n",
    "mae = mean_absolute_error(all_labels, all_predictions)\n",
    "\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a2592-8aa4-4bc1-a935-ef40f4718a65",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "26e065ef-9f27-4d5f-9c15-e0876c6552e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "pred_flat = [item.item() for sublist in all_predictions for item in sublist]\n",
    "true_flat = [item.item() for sublist in all_labels for item in sublist]\n",
    "\n",
    "eval_data = pd.DataFrame({\n",
    "    'Predicted_PCE': pred_flat,\n",
    "    'Actual_PCE': true_flat\n",
    "})\n",
    "\n",
    "g = sns.jointplot(x=\"Predicted_PCE\", y=\"Actual_PCE\", data=eval_data,\n",
    "                  kind=\"reg\", truncate=False,\n",
    "                  xlim=(0, 25), ylim=(0, 25),\n",
    "                  color=\"m\", height=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94883bb8-83af-43fa-ab7c-b2ca656c0f71",
   "metadata": {},
   "source": [
    "# Save predictions for ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eeb8a124-dbc9-4a7b-b75e-5e781309b438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want a flat list of floats\n",
    "all_predictions_save = [x.item() for x in all_predictions]\n",
    "all_labels_save = [x.item() for x in all_labels]\n",
    "\n",
    "GNN_predictions = pd.DataFrame()\n",
    "GNN_predictions['predicted'] = all_predictions_save\n",
    "GNN_predictions['true_values'] = all_labels_save\n",
    "\n",
    "# Change name here to reflect which model was saved\n",
    "GNN_predictions.to_csv('data_RO2/GNN_full_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
